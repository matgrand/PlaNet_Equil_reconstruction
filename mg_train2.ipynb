{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")  # adds seaborn style to charts, eg. grid\n",
    "plt.style.use(\"dark_background\")  # inverts colors to dark theme\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import calc_gso_batch # gso/pinn calculation\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    device = torch.device(\"mps\") # apple silicon\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "os.makedirs(f\"mg_data/{JOBID}\", exist_ok=True)\n",
    "print(f'device: {device}')\n",
    "\n",
    "# copy the python training to the directory (for cluster) (for local, it fails silently)\n",
    "os.system(f\"cp mg_train2.py mg_data/{JOBID}/mg_train2.py\")\n",
    "os.system(f\"cp utils.py mg_data/{JOBID}/utils.py\")\n",
    "\n",
    "def to_tensor(x, device=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=device)\n",
    "\n",
    "SMALL, NORM, BIG = \"small\", \"norm\", \"big\"\n",
    "PRENAME_MSE, PRENAME_GSO, PRENAME_TOT = \"mg_planet_mse\", \"mg_planet_gso\", \"mg_planet_tot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"mg_data/{JOBID}\" \n",
    "EPOCHS = 1000 # number of epochs, note: needs to \n",
    "BATCH_SIZE = 128 # 128 best\n",
    "\n",
    "LOAD_PRETRAINED = None # Set it to None if you don't want to load pretrained model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_1809761.pth\" # norm model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_small_1810888.pth\" # small model\n",
    "# LOAD_PRETRAINED = \"trained_models/pretrained_big_1811142.pth\" # big model\n",
    "\n",
    "# LEARNING_RATE = 3e-4*np.linspace(1, 1e-2, EPOCHS)  # best\n",
    "LEARNING_RATE = 3e-4*np.logspace(0, -2, EPOCHS)\n",
    "# LEARNING_RATE = 1e-4*np.logspace(0, -2, EPOCHS)\n",
    "\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.1, EPOCHS) # best\n",
    "# GSO_LOSS_RATIO = np.linspace(0.3, 0.1, EPOCHS) # best too\n",
    "# GSO_LOSS_RATIO = np.linspace(0.4, 0.0, EPOCHS) # best for big model pretrain start\n",
    "GSO_LOSS_RATIO = np.concatenate((np.linspace(0.4, 0.0, EPOCHS//2), np.linspace(0.0, 0.0, EPOCHS//2))) \n",
    "# GSO_LOSS_RATIO = 0.1*np.ones(EPOCHS) # not very good\n",
    "# GSO_LOSS_RATIO = (0.5+0.5*np.sin(np.linspace(0, 25*np.pi, EPOCHS)))*np.linspace(1, 0.1, EPOCHS) # crazy\n",
    "\n",
    "NCURRS, NPROFS, NMAGS = 14, 202, 187 # input sizes\n",
    "INPUT_SIZE = NCURRS + NPROFS + NMAGS\n",
    "TRAIN_DS_PATH = \"data/train_ds.mat\" # generated from prepapre_dataset\n",
    "EVAL_DS_PATH = \"data/eval_ds.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "if LOAD_PRETRAINED is not None: assert os.path.exists(LOAD_PRETRAINED), \"Pretrained model does not exist\"\n",
    "assert os.path.exists(TRAIN_DS_PATH), \"Training dataset does not exist\"\n",
    "assert os.path.exists(EVAL_DS_PATH), \"Evaluation dataset does not exist\"\n",
    "assert os.path.exists(SAVE_DIR), \"Save directory does not exist\"\n",
    "assert len(LEARNING_RATE) == EPOCHS, \"Learning rate array length does not match epochs\"\n",
    "assert len(GSO_LOSS_RATIO) == EPOCHS, \"GSO loss ratio array length does not match epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best train runs: \n",
    "| ID       | NET | MSE    | GS0    | Pre | Notes |\n",
    "|----------|-----|--------|--------|-----|-------|\n",
    "| 1810019  | norm | 0.0072 | 0.0730 | / | / |\n",
    "| 1809989  | norm | 0.0082 | 0.3061 | / | / |\n",
    "| 1809986  | norm | 0.0050 | 0.0727 | / | / |\n",
    "| 1809768  | norm | 0.0030 | 0.0500 | / | / |\n",
    "| 1809761  | norm | 0.0060 | 0.0840 | / | / |\n",
    "| 1810294  | norm | 0.0026 | 0.0487 | 1809768 | pre-trained from 1809768 |\n",
    "| 1810825  | small | 0.0265 | 0.2606 | / | small model |\n",
    "| 1810888  | small | 0.0234 | 0.2236 | / | has potential for improvement |\n",
    "| 1810897  | small | 0.0388 | 0.2940 | / | start lr from 1e-4, bs 64| \n",
    "| 1810903  | norm | 0.0232 | 0.1345 | / | 0.1 const gso ratio (bad?) |\n",
    "| 1811116  | norm | 0.0141 | 0.0768 | / | 0.3 const gso ratio |\n",
    "| 1811117  | big | 0.0024 | 0.0601 | / | 0.1 const gso ratio, lr dec 3e-4 | \n",
    "| 1811142  | big | 0.0019 | 0.0535 | / | 0.1 const gso ratio, lr dec 3e-4 |\n",
    "| 1811143  | big | 0.0020 | 0.0609 | / | exact same as 1811142 |\n",
    "| 1811302 | big | 0.0063 | 0.0500 | / | gso .3 -> .1, lr dec 3e-4 |\n",
    "| 1811304 | big | 0.0013 | 0.0310 | 1811142 | gso .3 -> .1, lr dec 3e-4, first time train loss < eval |\n",
    "| 1814866 | big | 0.0012 | 0.0304 | 1811142 | gso .3 -> 0.0 \n",
    "| 1814867 | big | 0.0012 | **0.0277** | 1811142 | gso .3 -> 0.0, repeat 1814866 |\n",
    "| 1817256 | big | 0.0022 | 0.0512 | 1811142 | batch size 256 (bad) |\n",
    "| 1817333 | big | **0.0009** | 0.0333 | 1811142 | same as 1814866, but keep 0.0 from ep 500-1000 |\n",
    "| 1823444 | big | 0.0014 | 0.0579 | / | same as 1817333, but from scratch |\n",
    "|||||||\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate [Log]\")\n",
    "ax[0].plot(LEARNING_RATE, color=\"red\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO, color=\"red\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Measurement    | Mean        | Standard Deviation |\n",
    "|----------------|-------------|--------------------|\n",
    "| Current        | -10183.76   | 34209.11           |\n",
    "| Magnetic       | -0.20       | 0.58               |\n",
    "| F Profile      | 33.13       | 0.28               |\n",
    "| P Profile      | 9654.42     | 8788.29            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_mat_path):\n",
    "        ds_mat = sio.loadmat(ds_mat_path)\n",
    "        # output: magnetic flux, transposed (matlab is column-major)\n",
    "        self.psi = to_tensor(ds_mat[\"psi\"]).view(-1, 1, 64, 64)\n",
    "        # inputs: radial and vertical position of pixels (for plotting only rn) + currents + measurements + profiles \n",
    "        self.rr = to_tensor(ds_mat[\"rr\"]).view(-1,1,64,64) # radial position of pixels (64, 64)\n",
    "        self.zz = to_tensor(ds_mat[\"zz\"]).view(-1,1,64,64) # vertical position of pixels (64, 64)\n",
    "        self.currs = ds_mat[\"currs\"] # input currents (n, 14)\n",
    "        self.mags = ds_mat[\"magnetic\"] # input magnetic measurements (n, 187)\n",
    "        f_prof = ds_mat[\"f_profiles\"] # input profiles (n, 101)\n",
    "        p_prof = ds_mat[\"p_profiles\"] # input profiles (n, 101)\n",
    "        self.currs = (to_tensor(self.currs)+10183)/34209 # (n, 14) # normalized\n",
    "        self.mags = (to_tensor(self.mags)+0.2)/0.58 # (n, 187) # normalized\n",
    "        self.profs = torch.cat(((to_tensor(f_prof)-33.13)/0.28, (to_tensor(p_prof)-9654)/8788), 1) # (n, 202) # normalized\n",
    "        # move to device (doable bc the dataset is fairly small, check memory usage)\n",
    "        self.currs, self.mags, self.profs = self.currs.to(device), self.mags.to(device), self.profs.to(device)\n",
    "        self.psi, self.rr, self.zz = self.psi.to(device), self.rr.to(device), self.zz.to(device)\n",
    "        self.everything = [self.currs, self.mags, self.profs, self.psi, self.rr, self.zz]\n",
    "        print(f\"Dataset: {len(self)}, memory: {sum([x.element_size()*x.nelement() for x in self.everything])/1024**2:.0f} MB\")\n",
    "    def __len__(self): return len(self.psi)\n",
    "    def __getitem__(self, idx): return [x[idx] for x in self.everything]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(np.random.randint(0, len(ds), n_plot)):\n",
    "    psi, rr, zz = ds[j][3].cpu().numpy().squeeze(), ds[j][4].cpu().numpy().squeeze(), ds[j][5].cpu().numpy().squeeze()\n",
    "    axs[i].contourf(rr, zz, psi, 100, cmap=\"inferno\")\n",
    "    axs[i].contour(rr, zz, -psi, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/dataset.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network architecture\n",
    "![|100](mg_data/planet_eq_net.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: PlaNet: # Paper net: branch + trunk conenction and everything \n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, ConvTranspose2d\n",
    "Λ = ReLU() # activation function\n",
    "class Head(Module): \n",
    "    def __init__(self):\n",
    "        super(Head, self).__init__()\n",
    "        self.fc = Sequential(Linear(64, 4096), Λ)\n",
    "        self.anti_conv = Sequential( # U-Net style\n",
    "            ConvTranspose2d(64, 64, kernel_size=2, stride=2), \n",
    "            Conv2d(64, 64, kernel_size=3, padding=0), Λ,\n",
    "            Conv2d(64, 64, kernel_size=3, padding=0), Λ,\n",
    "            ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            Conv2d(32, 32, kernel_size=3, padding=0), Λ,\n",
    "            Conv2d(32, 32, kernel_size=3, padding=0), Λ,\n",
    "            ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            Conv2d(16, 16, kernel_size=3, padding=0), Λ,\n",
    "            Conv2d(16, 16, kernel_size=3, padding=0), Λ,\n",
    "            ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "            Conv2d(8, 4, kernel_size=3, padding=0), Λ,\n",
    "            Conv2d(4, 2, kernel_size=3, padding=0), Λ,\n",
    "            Conv2d(2, 1, kernel_size=5, padding=0),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 64, 8, 8)\n",
    "        x = self.anti_conv(x)\n",
    "        return x\n",
    "    def save(self, dir, prename): torch.save(self.state_dict(), f\"{dir}/{prename}_head.pth\")\n",
    "    def load(self, dir, prename): self.load_state_dict(torch.load(f\"{dir}/{prename}_head.pth\"))\n",
    "    \n",
    "##########################################################################################\n",
    "class Trunk(Module): \n",
    "    def __init__(self):\n",
    "        super(Trunk, self).__init__()\n",
    "        def trunk_block(): \n",
    "            return  Sequential(\n",
    "                Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), Λ, MaxPool2d(2),\n",
    "                Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), Λ, MaxPool2d(2),\n",
    "                Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), Λ, MaxPool2d(2),\n",
    "            )\n",
    "        self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "        self.trunk_fc = Sequential(\n",
    "            Linear(2*32*8*8, 128), Λ,\n",
    "            Linear(128, 64), Λ,\n",
    "            Linear(64, 64), Λ, \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        r, z = x # split inputs\n",
    "        r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "        r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "        xt = torch.cat((r, z), 1) # concatenate\n",
    "        xt = self.trunk_fc(xt) # fully connected\n",
    "        return xt\n",
    "    def save(self, dir, prename): torch.save(self.state_dict(), f\"{dir}/{prename}_trunk.pth\")\n",
    "    def load(self, dir, prename): self.load_state_dict(torch.load(f\"{dir}/{prename}_trunk.pth\"))\n",
    "    \n",
    "##########################################################################################\n",
    "class Branch(Module): \n",
    "    def __init__(self, input_size):\n",
    "        super(Branch, self).__init__()\n",
    "        self.branch = Sequential(\n",
    "            Linear(input_size, 256), Λ,\n",
    "            Linear(256, 128), Λ,\n",
    "            Linear(128, 64), Λ\n",
    "        )\n",
    "    def forward(self, xb): return self.branch(xb)\n",
    "    def save(self, dir, prename): torch.save(self.state_dict(), f\"{dir}/{prename}_branch.pth\")\n",
    "    def load(self, dir, prename): self.load_state_dict(torch.load(f\"{dir}/{prename}_branch.pth\"))\n",
    "\n",
    "##########################################################################################\n",
    "class PlaNet(Module):\n",
    "    def __init__(self, branch:Branch, trunk:Trunk, head:Head):\n",
    "        super(PlaNet, self).__init__()\n",
    "        assert isinstance(branch, Branch) and isinstance(trunk, Trunk) and isinstance(head, Head)\n",
    "        self.branch, self.trunk, self.head = branch, trunk, head\n",
    "    def forward(self, x):\n",
    "        xb, r, z = x # split inputs\n",
    "        xb = self.branch(xb) # branch\n",
    "        xt = self.trunk((r, z)) # trunk\n",
    "        x = xb * xt # element-wise multiplication\n",
    "        x = self.head(x) # head\n",
    "        return x\n",
    "    def save(self, dir, prename): \n",
    "        self.branch.save(dir, prename)\n",
    "        self.trunk.save(dir, prename)\n",
    "        self.head.save(dir, prename)\n",
    "    def load(self, dir, prename):\n",
    "        self.branch.load(dir, prename)\n",
    "        self.trunk.load(dir, prename)\n",
    "        self.head.load(dir, prename)\n",
    "        print(f\"Model loaded from {dir}/{prename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input/output shapes\n",
    "x = (torch.rand(1, INPUT_SIZE), torch.rand(1, 1, 64, 64), torch.rand(1, 1, 64, 64))\n",
    "net = PlaNet(Branch(INPUT_SIZE), Trunk(), Head())\n",
    "y = net(x)\n",
    "print(f\"in: {[x.shape for x in x]}, out: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx = (torch.rand(n_sampl, INPUT_SIZE), torch.rand(n_sampl, 1, 64, 64), torch.rand(n_sampl, 1, 64, 64))\n",
    "ny = net(nx)\n",
    "print(f\"in: {[x.shape for x in nx]}, out: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, 64, 64), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "def train():\n",
    "    train_ds, val_ds = PlaNetDataset(TRAIN_DS_PATH), PlaNetDataset(EVAL_DS_PATH) # initialize datasets\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "    trunk, head = Trunk(), Head() # initialize modules (common modules)\n",
    "    branch1, branch2, branch3 = Branch(NCURRS+NMAGS+NPROFS), Branch(NCURRS+NMAGS), Branch(NCURRS) # 3 different branches\n",
    "    model1 = PlaNet(branch1, trunk, head) # initialize model 1 \n",
    "    model2 = PlaNet(branch2, trunk, head) # initialize model 2\n",
    "    model3 = PlaNet(branch3, trunk, head) # initialize model 3\n",
    "    if LOAD_PRETRAINED is not None: raise NotImplementedError(\"Pretrained model loading is not implemented yet\")\n",
    "    model1.to(device), model2.to(device), model3.to(device) # move to device\n",
    "    optimizer = torch.optim.Adam(list(model1.parameters()) + list(model2.parameters()) + list(model3.parameters()), lr=LEARNING_RATE[0]) # optimizer\n",
    "    loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "    tlog_mse1, tlog_gso1, elog_mse1, elog_gso1 = [], [], [], [] # logs for losses\n",
    "    tlog_mse2, tlog_gso2, elog_mse2, elog_gso2 = [], [], [], [] # logs for losses\n",
    "    tlog_mse3, tlog_gso3, elog_mse3, elog_gso3 = [], [], [], [] # logs for losses\n",
    "    start_time = time() # start time\n",
    "    for ep in range(EPOCHS): \n",
    "        epoch_time = time()\n",
    "        for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "        model1.train(), model2.train() # training mode\n",
    "        trainloss, evalloss = [], []\n",
    "        trainloss1, evalloss1 = [], []\n",
    "        trainloss2, evalloss2 = [], []\n",
    "        trainloss3, evalloss3 = [], []\n",
    "        for curr, mag, prof, psi, rr, zz in train_dl:\n",
    "            input1, input2, input3 = torch.cat((curr, mag, prof), 1), torch.cat((curr, mag), 1), curr # concatenate inputs\n",
    "            optimizer.zero_grad() # zero gradients\n",
    "            psi_pred1 = model1((input1, rr, zz)) # forward pass\n",
    "            psi_pred2 = model2((input2, rr, zz))\n",
    "            psi_pred3 = model3((input3, rr, zz))\n",
    "            gso = calc_gso_batch(psi, rr, zz, dev=device) # calculate grad shafranov\n",
    "            gso_pred1 = calc_gso_batch(psi_pred1, rr, zz, dev=device)\n",
    "            gso_pred2 = calc_gso_batch(psi_pred2, rr, zz, dev=device)\n",
    "            gso_pred3 = calc_gso_batch(psi_pred3, rr, zz, dev=device)\n",
    "            mse_loss1, gso_loss1 = loss_fn(psi_pred1, psi), loss_fn(gso_pred1, gso) # losses\n",
    "            mse_loss2, gso_loss2 = loss_fn(psi_pred2, psi), loss_fn(gso_pred2, gso) \n",
    "            mse_loss3, gso_loss3 = loss_fn(psi_pred3, psi), loss_fn(gso_pred3, gso) \n",
    "            mse_loss, gso_loss = (mse_loss1+mse_loss2+mse_loss3)/3, (gso_loss1+gso_loss2+gso_loss3)/3 # average losses\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            loss.backward() # backprop\n",
    "            optimizer.step() # update weights\n",
    "            trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "            trainloss1.append((mse_loss1.item(), gso_loss1.item())) \n",
    "            trainloss2.append((mse_loss2.item(), gso_loss2.item())) \n",
    "            trainloss3.append((mse_loss3.item(), gso_loss3.item()))\n",
    "        model1.eval(), model2.eval(), model3.eval() # evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for curr, mag, prof, psi, rr, zz in val_dl:\n",
    "                input1, input2, input3 = torch.cat((curr, mag, prof), 1), torch.cat((curr, mag), 1), curr # concatenate inputs\n",
    "                psi_pred1 = model1((input1, rr, zz))\n",
    "                psi_pred2 = model2((input2, rr, zz))\n",
    "                psi_pred3 = model3((input3, rr, zz))\n",
    "                gso = calc_gso_batch(psi, rr, zz, dev=device)\n",
    "                gso_pred1 = calc_gso_batch(psi_pred1, rr, zz, dev=device)\n",
    "                gso_pred2 = calc_gso_batch(psi_pred2, rr, zz, dev=device)\n",
    "                gso_pred3 = calc_gso_batch(psi_pred3, rr, zz, dev=device)\n",
    "                mse_loss1, gso_loss1 = loss_fn(psi_pred1, psi), loss_fn(gso_pred1, gso)\n",
    "                mse_loss2, gso_loss2 = loss_fn(psi_pred2, psi), loss_fn(gso_pred2, gso)\n",
    "                mse_loss3, gso_loss3 = loss_fn(psi_pred3, psi), loss_fn(gso_pred3, gso)\n",
    "                mse_loss, gso_loss = (mse_loss1+mse_loss2+mse_loss3)/3, (gso_loss1+gso_loss2+gso_loss3)/3\n",
    "                evalloss.append((mse_loss.item(), gso_loss.item()))\n",
    "                evalloss1.append((mse_loss1.item(), gso_loss1.item()))\n",
    "                evalloss2.append((mse_loss2.item(), gso_loss2.item()))\n",
    "                evalloss3.append((mse_loss3.item(), gso_loss3.item()))\n",
    "                \n",
    "        tloss_tot, tloss_mse, tloss_gso = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "        tloss_mse1, tloss_gso1 = map(lambda x: sum(x)/len(x), zip(*trainloss1))\n",
    "        tloss_mse2, tloss_gso2 = map(lambda x: sum(x)/len(x), zip(*trainloss2))\n",
    "        tloss_mse3, tloss_gso3 = map(lambda x: sum(x)/len(x), zip(*trainloss3))\n",
    "        eloss_mse, eloss_gso   = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "        eloss_mse1, eloss_gso1 = map(lambda x: sum(x)/len(x), zip(*evalloss1))\n",
    "        eloss_mse2, eloss_gso2 = map(lambda x: sum(x)/len(x), zip(*evalloss2))\n",
    "        eloss_mse3, eloss_gso3 = map(lambda x: sum(x)/len(x), zip(*evalloss3))\n",
    "\n",
    "        # save model if improved        \n",
    "        endp1 = endp2 = endp3 = \"\\n\" \n",
    "        if eloss_mse1 <= min(elog_mse1, default=eloss_mse1):\n",
    "            model1.save(SAVE_DIR, f\"{PRENAME_MSE}_1\"); endp1=\" *mse1\\n\"\n",
    "        if eloss_gso1 <= min(elog_gso1, default=eloss_gso1):\n",
    "            model1.save(SAVE_DIR, f\"{PRENAME_GSO}_1\"); endp1=\" *gso1\\n\"\n",
    "        if eloss_mse2 <= min(elog_mse2, default=eloss_mse2):\n",
    "            model2.save(SAVE_DIR, f\"{PRENAME_MSE}_2\"); endp2=\" *mse2\\n\"\n",
    "        if eloss_gso2 <= min(elog_gso2, default=eloss_gso2):\n",
    "            model2.save(SAVE_DIR, f\"{PRENAME_GSO}_2\"); endp2=\" *gso2\\n\"\n",
    "        if eloss_mse3 <= min(elog_mse3, default=eloss_mse3):\n",
    "            model3.save(SAVE_DIR, f\"{PRENAME_MSE}_3\"); endp3=\" *mse3\\n\"\n",
    "        if eloss_gso3 <= min(elog_gso3, default=eloss_gso3):\n",
    "            model3.save(SAVE_DIR, f\"{PRENAME_GSO}_3\"); endp3=\" *gso3\\n\"\n",
    "        tlog_mse1.append(tloss_mse1); tlog_gso1.append(tloss_gso1)\n",
    "        elog_mse1.append(eloss_mse1); elog_gso1.append(eloss_gso1)\n",
    "        tlog_mse2.append(tloss_mse2); tlog_gso2.append(tloss_gso2)\n",
    "        elog_mse2.append(eloss_mse2); elog_gso2.append(eloss_gso2)\n",
    "        tlog_mse3.append(tloss_mse3); tlog_gso3.append(tloss_gso3)\n",
    "        elog_mse3.append(eloss_mse3); elog_gso3.append(eloss_gso3)\n",
    "        print(f\"{ep+1}/{EPOCHS}\\n1: Eval: mse {eloss_mse1:.4f}, gso {eloss_gso1:.4f} | lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | \" + \n",
    "            f\"{time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp1,  flush=True)\n",
    "        print(f\"2: Eval: mse {eloss_mse2:.4f}, gso {eloss_gso2:.4f} | lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | \" + \n",
    "            f\"{time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp2,  flush=True)\n",
    "        print(f\"3: Eval: mse {eloss_mse3:.4f}, gso {eloss_gso3:.4f} | lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | \" +\n",
    "            f\"{time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp3,  flush=True)\n",
    "        if ep >= 10 and (eloss_gso > 30.0 or eloss_mse > 11.0): return False, (), () # stop training, if not converging, try again\n",
    "    print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "    print(f\"Best losses 1: mse1 {min(elog_mse1):.4f}, gso1 {min(elog_gso1):.4f}\")\n",
    "    print(f\"Best losses 2: mse1 {min(elog_mse2):.4f}, gso1 {min(elog_gso2):.4f}\")\n",
    "    print(f\"Best losses 3: mse1 {min(elog_mse3):.4f}, gso1 {min(elog_gso3):.4f}\")\n",
    "    for l, n in zip([tlog_mse1, tlog_gso1, tlog_mse2, tlog_gso2, tlog_mse3, tlog_gso3], [\"mse1\", \"gso1\", \"mse2\", \"gso2\", \"mse3\", \"gso3\"]): \n",
    "        np.save(f\"{SAVE_DIR}/train_{n}_losses.npy\", l) # save losses\n",
    "    for l, n in zip([elog_mse1, elog_gso1, elog_mse2, elog_gso2, elog_mse3, elog_gso3], [\"mse1\", \"gso1\", \"mse2\", \"gso2\", \"mse3\", \"gso3\"]): \n",
    "        np.save(f\"{SAVE_DIR}/eval_{n}_losses.npy\", l) # save losses\n",
    "    logs = [tlog_mse1, tlog_gso1, tlog_mse2, tlog_gso2, tlog_mse3, tlog_gso3, elog_mse1, elog_gso1, elog_mse2, elog_gso2, elog_mse3, elog_gso3]\n",
    "    return True, (model1, model2, model3), logs\n",
    "\n",
    "# train the model (multiple attempts)\n",
    "for i in range(10): \n",
    "    success, (model1, model2, model3), logs = train()\n",
    "    if success: tlog_mse1,tlog_gso1,tlog_mse2,tlog_gso2,tlog_mse3,tlog_gso3,elog_mse1,elog_gso1,elog_mse2,elog_gso2,elog_mse3,elog_gso3=logs; break\n",
    "    else: print(f\"Convergence failed, retrying... {i+1}/10\")\n",
    "assert success, \"Training failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "all_mse = [tlog_mse1, elog_mse1, tlog_mse2, elog_mse2, tlog_mse3, elog_mse3]\n",
    "all_gso = [tlog_gso1, elog_gso1, tlog_gso2, elog_gso2, tlog_gso3, elog_gso3]\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 10))\n",
    "ce, ct = \"yellow\", \"red\"\n",
    "lw = 1.0\n",
    "ax[0,0].set_title(\"MSE Loss 1\")\n",
    "ax[0,0].plot(tlog_mse1, color=ct, label=\"train 1\", linewidth=lw)\n",
    "ax[0,0].plot(elog_mse1, color=ce, label=\"eval 1\", linewidth=lw)\n",
    "ax[0,1].set_title(\"GSO Loss 1\")\n",
    "ax[0,1].plot(tlog_gso1, color=ct, label=\"train 1\", linewidth=lw)\n",
    "ax[0,1].plot(elog_gso1, color=ce, label=\"eval 1\", linewidth=lw)\n",
    "ax[1,0].set_title(\"MSE Loss 2\")\n",
    "ax[1,0].plot(tlog_mse2, color=ct, label=\"train 2\", linewidth=lw)\n",
    "ax[1,0].plot(elog_mse2, color=ce, label=\"eval 2\", linewidth=lw)\n",
    "ax[1,1].set_title(\"GSO Loss 2\")\n",
    "ax[1,1].plot(tlog_gso2, color=ct, label=\"train 2\", linewidth=lw)\n",
    "ax[1,1].plot(elog_gso2, color=ce, label=\"eval 2\", linewidth=lw)\n",
    "ax[2,0].set_title(\"MSE Loss 3\")\n",
    "ax[2,0].plot(tlog_mse3, color=ct, label=\"train 3\", linewidth=lw)\n",
    "ax[2,0].plot(elog_mse3, color=ce, label=\"eval 3\", linewidth=lw)\n",
    "ax[2,1].set_title(\"GSO Loss 3\")\n",
    "ax[2,1].plot(tlog_gso3, color=ct, label=\"train 3\", linewidth=lw)\n",
    "ax[2,1].plot(elog_gso3, color=ce, label=\"eval 3\", linewidth=lw)\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss\")\n",
    "for a in ax[:,0]: a.set_ylim(min([min(x) for x in all_mse]), max([max(x) for x in all_mse]))\n",
    "for a in ax[:,1]: a.set_ylim(min([min(x) for x in all_gso]), max([max(x) for x in all_gso]))\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")\n",
    "#now the same but with log scale\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 10))\n",
    "ax[0,0].set_title(\"MSE Loss 1 (log)\")\n",
    "ax[0,0].plot(tlog_mse1, color=ct, label=\"train 1\", linewidth=lw)\n",
    "ax[0,0].plot(elog_mse1, color=ce, label=\"eval 1\", linewidth=lw)\n",
    "ax[0,1].set_title(\"GSO Loss 1 (log)\")\n",
    "ax[0,1].plot(tlog_gso1, color=ct, label=\"train 1\", linewidth=lw)\n",
    "ax[0,1].plot(elog_gso1, color=ce, label=\"eval 1\", linewidth=lw)\n",
    "ax[1,0].set_title(\"MSE Loss 2 (log)\")\n",
    "ax[1,0].plot(tlog_mse2, color=ct, label=\"train 2\", linewidth=lw)\n",
    "ax[1,0].plot(elog_mse2, color=ce, label=\"eval 2\", linewidth=lw)\n",
    "ax[1,1].set_title(\"GSO Loss 2 (log)\")\n",
    "ax[1,1].plot(tlog_gso2, color=ct, label=\"train 2\", linewidth=lw)\n",
    "ax[1,1].plot(elog_gso2, color=ce, label=\"eval 2\", linewidth=lw)\n",
    "ax[2,0].set_title(\"MSE Loss 3 (log)\")\n",
    "ax[2,0].plot(tlog_mse3, color=ct, label=\"train 3\", linewidth=lw)\n",
    "ax[2,0].plot(elog_mse3, color=ce, label=\"eval 3\", linewidth=lw)\n",
    "ax[2,1].set_title(\"GSO Loss 3 (log)\")\n",
    "ax[2,1].plot(tlog_gso3, color=ct, label=\"train 3\", linewidth=lw)\n",
    "ax[2,1].plot(elog_gso3, color=ce, label=\"eval 3\", linewidth=lw)\n",
    "for a in ax.flatten(): a.legend(); a.set_xlabel(\"Epoch\"); a.set_ylabel(\"Loss [log]\"); a.grid(True, which=\"both\", axis=\"y\"); a.set_yscale(\"log\")\n",
    "for a in ax[:,0]: a.set_ylim(min([min(x) for x in all_mse]), max([max(x) for x in all_mse]))\n",
    "for a in ax[:,1]: a.set_ylim(min([min(x) for x in all_gso]), max([max(x) for x in all_gso]))\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses_log.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "cpu_times1, cpu_times2, dev_times1, dev_times2 = [], [], [], []\n",
    "for tit, prename in zip([\"MSE1\", \"GSO1\", \"MSE2\", \"GSO2\"], [f\"{PRENAME_MSE}_1\", f\"{PRENAME_GSO}_1\", f\"{PRENAME_MSE}_2\", f\"{PRENAME_GSO}_2\"]):\n",
    "    is_model_1 = \"1\" in tit\n",
    "    branch = model1.branch if is_model_1 else model2.branch\n",
    "    trunk, head = model1.trunk, model1.head\n",
    "    assert model1.trunk == model2.trunk == model3.trunk\n",
    "    assert model1.head == model2.head == model3.head\n",
    "    trunk.load(SAVE_DIR, prename), head.load(SAVE_DIR, prename), branch.load(SAVE_DIR, prename)\n",
    "    model = PlaNet(branch, trunk, head)\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "    # ds = PlaNetDataset(TRAIN_DS_PATH)\n",
    "    os.makedirs(f\"mg_data/{JOBID}/imgs\", exist_ok=True)\n",
    "    N_PLOTS = 2 if HAS_SCREEN else 50\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 9))\n",
    "        curr, mag, prof, psi_ds, rr, zz = [d.to(\"cpu\") for d in ds[i]]\n",
    "        curr, mag, prof, psi_ds, rr, zz = curr.view(1,-1), mag.view(1,-1), prof.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        input = torch.cat((curr, mag, prof), 1) if is_model_1 else torch.cat((curr, mag), 1)\n",
    "        # dev\n",
    "        model.to(device)\n",
    "        input, rr, zz = input.to(device), rr.to(device), zz.to(device)\n",
    "        start = time()\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        end = time()\n",
    "        if is_model_1: dev_times1.append(end-start) \n",
    "        else: dev_times2.append(end-start)\n",
    "        # cpu\n",
    "        model.to(\"cpu\")\n",
    "        input, rr, zz = input.to(\"cpu\"), rr.to(\"cpu\"), zz.to(\"cpu\")\n",
    "        start = time()\n",
    "        psi_pred = model((input, rr, zz))\n",
    "        end = time()\n",
    "        if is_model_1: cpu_times1.append(end-start) \n",
    "        else: cpu_times2.append(end-start)\n",
    "\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([]), ax.set_aspect(\"equal\")\n",
    "        plt.suptitle(f\"PlaNet: {tit} {i}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/imgs/planet_{tit}_{i}.png\")\n",
    "        plt.close()\n",
    "cpu_times1, cpu_times2, dev_times1, dev_times2 = map(np.array, [cpu_times1, cpu_times2, dev_times1, dev_times2])\n",
    "print(f\"cpu: inference time 1: {cpu_times1.mean():.5f}s, std: {cpu_times1.std():.5f}\")\n",
    "print(f\"cpu: inference time 2: {cpu_times2.mean():.5f}s, std: {cpu_times2.std():.5f}\")\n",
    "print(f\"dev: inference time 1: {dev_times1.mean():.5f}s, std: {dev_times1.std():.5f}\")\n",
    "print(f\"dev: inference time 3: {dev_times2.mean():.5f}s, std: {dev_times2.std():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\", flush=True)\n",
    "if not HAS_SCREEN: sleep(30) # wait for files to update (for cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt mg_data/{JOBID}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
