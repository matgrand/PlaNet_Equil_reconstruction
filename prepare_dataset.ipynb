{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset from the equilibria dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "from os.path import join, exists\n",
    "from utils import resample_on_new_subgrid, interp_fun, get_box_from_grid, sample_random_subgrid, calc_laplace_df_dr_ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "DTYPE = 'float32'\n",
    "DATA_DIR = \"data\" # where the data is stored\n",
    "full_ds_mat_path = join(DATA_DIR, 'ITER_like_equilibrium_dataset.mat')\n",
    "sample_ds_mat_path = join(DATA_DIR, 'ITER_like_equilibrium_dataset_sample.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download datasets from gdrive, # uncomment if you want to download the dataset\n",
    "# import gdown\n",
    "# gdown.download(id=\"1-5KP7_OYIvDD_QXvIr5sDihVxZx1qJCN\", output=full_ds_mat_path, quiet=False)\n",
    "# gdown.download(id=\"1Gn_OrMzxPRkTk-i77--HiWmWZyd8i8ue\", output=sample_ds_mat_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all files and dirs in the data directory except for full_ds_mat_path and sample_ds_mat_path\n",
    "if not exists(DATA_DIR): os.makedirs(DATA_DIR) # create data directory \n",
    "for file_or_dir in os.listdir(DATA_DIR):\n",
    "    fn = os.path.join(DATA_DIR, file_or_dir)\n",
    "    if os.path.isfile(fn) and fn not in [full_ds_mat_path, sample_ds_mat_path]: os.remove(fn)\n",
    "    elif os.path.isdir(fn): \n",
    "        for root, dirs, files in os.walk(fn, topdown=False):\n",
    "            for name in files: os.remove(os.path.join(root, name))\n",
    "            for name in dirs: os.rmdir(os.path.join(root, name))\n",
    "        os.rmdir(fn)\n",
    "    else: print(f\"Skipping {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from mat file\n",
    "mat_ds = io.loadmat(full_ds_mat_path)\n",
    "DB_psi_pixel_test_ConvNet = mat_ds['DB_psi_pixel_test_ConvNet'].astype(DTYPE)\n",
    "DB_meas_Bpickup_test_ConvNet = mat_ds['DB_meas_Bpickup_test_ConvNet'].astype(DTYPE)\n",
    "DB_coils_curr_test_ConvNet = mat_ds['DB_coils_curr_test_ConvNet'].astype(DTYPE)\n",
    "DB_p_test_ConvNet = mat_ds['DB_p_test_ConvNet'].astype(DTYPE)\n",
    "RRf = mat_ds['RR_pixels'].astype(DTYPE) # radial coordinate f=Full grid\n",
    "ZZf = mat_ds['ZZ_pixels'].astype(DTYPE) # vertical coordinate f=Full grid\n",
    "DB_res_RHS_pixel_test_ConvNet = mat_ds['DB_res_RHS_pixel_test_ConvNet'].astype(DTYPE)\n",
    "DB_separatrix_200_test_ConvNet = mat_ds['DB_separatrix_200_test_ConvNet'].astype(DTYPE)\n",
    "DB_Jpla_pixel_test_ConvNet = mat_ds['DB_Jpla_pixel_test_ConvNet'].astype(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset input and output\n",
    "X = np.column_stack((DB_meas_Bpickup_test_ConvNet, DB_coils_curr_test_ConvNet)) #, DB_f_test_ConvNet, DB_p_test_ConvNet ))\n",
    "Y = DB_psi_pixel_test_ConvNet\n",
    "RHS = DB_res_RHS_pixel_test_ConvNet\n",
    "N_ORIG = len(X) # number of samples in the original dataset\n",
    "print(f\"Original dataset size: {N_ORIG}\")\n",
    "# Save RRf, ZZf\n",
    "nr,nz = RRf.shape\n",
    "io.savemat(join(DATA_DIR, f'start_grid_{nr}x{nz}.mat'),{'RRf':RRf,'ZZf':ZZf})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataset example\n",
    "cmap = cm.inferno\n",
    "for i in range(0,1):\n",
    "    ind_plot = np.random.randint(0,N_ORIG,1)[0]\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 3), sharey=True)\n",
    "    ax0 = axs[0].contour(RRf,ZZf,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15)\n",
    "    fig.colorbar(ax0)\n",
    "    axs[0].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')\n",
    "    axs[0].axis('equal')\n",
    "    axs[0].set_xlabel('r [m]')\n",
    "    axs[0].set_ylabel('z [m]')\n",
    "    axs[0].set_title('Ψ [Wb] - equil. #{}'.format(ind_plot))\n",
    "    img = axs[1].contourf(RRf,ZZf,DB_Jpla_pixel_test_ConvNet[ind_plot,:,:],15)\n",
    "    fig.colorbar(img)\n",
    "    axs[1].axis('equal')\n",
    "    axs[1].set_title('$J_Ψ$ [A/m2] - equil. #{}'.format(ind_plot))\n",
    "    axs[1].set_xlabel('r [m]')\n",
    "    axs[1].set_ylabel('z [m]')\n",
    "    axs[1].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')\n",
    "\n",
    "    rm, rM, zm, zM = RRf.min(), RRf.max(), ZZf.min(), ZZf.max()\n",
    "    img = axs[2].contourf(RRf,ZZf,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15,cmap=cmap)\n",
    "    axs[2].plot( DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
    "    axs[2].set_xlim([rm,rM])\n",
    "    axs[2].set_ylim([zm,zM])\n",
    "    axs[2].axis('equal')\n",
    "    axs[2].set_axis_off()\n",
    "    axs[2].set_title('Ψ')\n",
    "    qq = DB_res_RHS_pixel_test_ConvNet[ind_plot,:,:]\n",
    "    img = axs[3].contourf(RRf,ZZf,qq,15,cmap=cmap)\n",
    "    axs[3].set_title('GS operator')\n",
    "    axs[3].axis('equal')\n",
    "    axs[3].set_xlim([rm,rM])\n",
    "    axs[3].set_ylim([zm,zM])\n",
    "    axs[3].axis('equal')\n",
    "    axs[3].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
    "    axs[3].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation on subgrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interpolation\n",
    "idx = np.random.randint(0, Y.shape[0], 1)[0]\n",
    "f, rhs = Y[idx,:,:], DB_res_RHS_pixel_test_ConvNet[idx,:,:]\n",
    "rrg, zzg = sample_random_subgrid(RRf,ZZf)\n",
    "box = get_box_from_grid(rrg, zzg)\n",
    "f_grid = interp_fun(Y[idx,:,:], RRf, ZZf, rrg, zzg)\n",
    "rhs_grid = interp_fun(rhs, RRf, ZZf, rrg, zzg)\n",
    "\n",
    "fig,ax = plt.subplots(1,5, figsize=(20,5))\n",
    "ax[0].scatter(RRf, ZZf, marker='.')\n",
    "ax[0].scatter(rrg, zzg, marker='.')\n",
    "ax[0].set_aspect('equal')\n",
    "\n",
    "im1 = ax[1].contourf(RRf, ZZf, f, 20)\n",
    "ax[1].plot(box[:,0],box[:,1])\n",
    "ax[1].set_aspect('equal')\n",
    "\n",
    "im2 = ax[2].contourf(rrg, zzg, f_grid, 20)\n",
    "ax[2].set_aspect('equal')\n",
    "\n",
    "im3 = ax[3].contourf(RRf, ZZf, rhs, 20)\n",
    "ax[3].set_aspect('equal')\n",
    "ax[3].plot(box[:,0],box[:,1])\n",
    "\n",
    "im4 = ax[4].contourf(rrg, zzg, rhs_grid, 20)\n",
    "ax[4].set_aspect('equal')\n",
    "\n",
    "plt.colorbar(im1,ax=ax[1])\n",
    "plt.colorbar(im2,ax=ax[2])\n",
    "plt.colorbar(im3,ax=ax[3])\n",
    "plt.colorbar(im4,ax=ax[4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N_SAMPLES = 50000 # number of samples to use for training\n",
    "TRAIN_EVAL_SPLIT = 0.8 # percentage of the dataset to use for training\n",
    "FULL_SUBGRID_SPLIT = 0.3 # percentage of the full grid \n",
    "N_GRID = 64 # number of points in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset splitting (N_TOP = original dataset size)\n",
    "NT = int(N_SAMPLES*TRAIN_EVAL_SPLIT)    # training\n",
    "NE = N_SAMPLES - NT                     # evaluation\n",
    "NTF = int(NT*FULL_SUBGRID_SPLIT)        # training full\n",
    "NTS = NT - NTF                          # training subgrid\n",
    "NEF = int(NE*FULL_SUBGRID_SPLIT)        # evaluation full\n",
    "NES = NE - NEF                          # evaluation subgrid \n",
    "assert NTF+NTS == NT\n",
    "assert NEF+NES == NE\n",
    "assert NTF + NTS + NEF + NES == N_SAMPLES\n",
    "print(f\"Training: {NT}, full: {NTF}, subgrid: {NTS}\")\n",
    "print(f\"Eval:     {NE}, full: {NEF}, subgrid: {NES}\")\n",
    "orig_idxs = np.random.permutation(N_ORIG)\n",
    "orig_idxs_train = orig_idxs[:int(N_ORIG*TRAIN_EVAL_SPLIT)] # original indices for training\n",
    "orig_idxs_eval = orig_idxs[int(N_ORIG*TRAIN_EVAL_SPLIT):] # original indices for evaluation\n",
    "# splitting the idxs\n",
    "idxs_tf = np.random.choice(orig_idxs_train, NTF, replace=False) # training full\n",
    "idxs_ts = np.random.choice(orig_idxs_train, NTS, replace=False) # can overlap with idxs_tf\n",
    "idxs_ef = np.random.choice(orig_idxs_eval, NEF, replace=False) # evaluation full\n",
    "idxs_es = np.random.choice(orig_idxs_eval, NES, replace=False) # can overlap with idxs_ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the dataset\n",
    "x_tf = np.zeros((NTF, *X[0].shape), dtype=DTYPE)\n",
    "y_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_tf = np.zeros((NTF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_ts = np.zeros((NTS, *X[0].shape), dtype=DTYPE)\n",
    "y_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_ts = np.zeros((NTS, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_ef = np.zeros((NEF, *X[0].shape), dtype=DTYPE)\n",
    "y_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_ef = np.zeros((NEF, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "x_es = np.zeros((NES, *X[0].shape), dtype=DTYPE)\n",
    "y_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rr_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "zz_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "rhs_es = np.zeros((NES, N_GRID, N_GRID), dtype=DTYPE)\n",
    "\n",
    "## fill the arrays\n",
    "# Train Full -> just copy the data\n",
    "x_tf[:], y_tf[:], rhs_tf[:] = X[idxs_tf], Y[idxs_tf], RHS[idxs_tf]\n",
    "rr_tf[:], zz_tf[:] = RRf, ZZf\n",
    "# Train Subgrid -> interpolate the data\n",
    "for i, idx in enumerate(tqdm(idxs_ts)):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRf, ZZf, N_GRID, N_GRID)\n",
    "    y_ts[i], rhs_ts[i], rr_ts[i], zz_ts[i] = yi, rhsi, rri, zzi\n",
    "x_ts[:] = X[idxs_ts]\n",
    "# Eval Full -> just copy the data\n",
    "x_ef[:], y_ef[:], rhs_ef[:] = X[idxs_ef], Y[idxs_ef], RHS[idxs_ef]\n",
    "rr_ef[:], zz_ef[:] = RRf, ZZf\n",
    "# Eval Subgrid -> interpolate the data\n",
    "for i, idx in enumerate(tqdm(idxs_es)):\n",
    "    (yi, rhsi), rri, zzi = resample_on_new_subgrid([Y[idx], RHS[idx]], RRf, ZZf, N_GRID, N_GRID)\n",
    "    y_es[i], rhs_es[i], rr_es[i], zz_es[i] = yi, rhsi, rri, zzi\n",
    "x_es[:] = X[idxs_es]\n",
    "\n",
    "# concatenate the arrays\n",
    "x_train = np.concatenate((x_tf, x_ts))\n",
    "y_train = np.concatenate((y_tf, y_ts))\n",
    "rr_train = np.concatenate((rr_tf, rr_ts))\n",
    "zz_train = np.concatenate((zz_tf, zz_ts))\n",
    "rhs_train = np.concatenate((rhs_tf, rhs_ts))\n",
    "del x_tf, y_tf, rr_tf, zz_tf, rhs_tf, x_ts, y_ts, rr_ts, zz_ts, rhs_ts\n",
    "\n",
    "x_eval = np.concatenate((x_ef, x_es))\n",
    "y_eval = np.concatenate((y_ef, y_es))\n",
    "rr_eval = np.concatenate((rr_ef, rr_es))\n",
    "zz_eval = np.concatenate((zz_ef, zz_es))\n",
    "rhs_eval = np.concatenate((rhs_ef, rhs_es))\n",
    "del x_ef, y_ef, rr_ef, zz_ef, rhs_ef, x_es, y_es, rr_es, zz_es, rhs_es\n",
    "print(f\"x_train: {x_train.shape}, y_train: {y_train.shape}, rr_train: {rr_train.shape}, zz_train: {zz_train.shape}, rhs_train: {rhs_train.shape}\")\n",
    "print(f\"x_eval: {x_eval.shape}, y_eval: {y_eval.shape}, rr_eval: {rr_eval.shape}, zz_eval: {zz_eval.shape}, rhs_eval: {rhs_eval.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kernels for Grad-Shafranov equation #TODO: double check this\n",
    "# so we don't have to do it during training\n",
    "laplace_ker_t = np.zeros((len(x_train), 3, 3), dtype=DTYPE)\n",
    "laplace_ker_e = np.zeros((len(x_eval), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_t = np.zeros((len(x_train), 3, 3), dtype=DTYPE)\n",
    "df_dr_ker_e = np.zeros((len(x_eval), 3, 3), dtype=DTYPE)\n",
    "hrs_t, hzs_t = rr_train[:,1,2]-rr_train[:,1,1], zz_train[:,2,1]-zz_train[:,1,1]\n",
    "hrs_e, hzs_e = rr_eval[:,1,2]-rr_eval[:,1,1], zz_eval[:,2,1]-zz_eval[:,1,1]\n",
    "for i in range(len(x_train)):\n",
    "    laplace_ker_t[i,:,:], df_dr_ker_t[i,:,:] = calc_laplace_df_dr_ker(hrs_t[i], hzs_t[i])\n",
    "for i in range(len(x_eval)):\n",
    "    laplace_ker_e[i,:,:], df_dr_ker_e[i,:,:] = calc_laplace_df_dr_ker(hrs_e[i], hzs_e[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset\n",
    "rows = 3\n",
    "idxs_train = np.random.randint(0, len(x_train), rows)\n",
    "idxs_eval = np.random.randint(0, len(x_eval), rows)\n",
    "fig,ax = plt.subplots(rows,6, figsize=(15,3*rows))\n",
    "box0 = get_box_from_grid(RRf, ZZf)\n",
    "for i, (it, ie)  in enumerate(zip(idxs_train, idxs_eval)):\n",
    "    # training\n",
    "    boxi = get_box_from_grid(rr_train[it], zz_train[it])\n",
    "    ax[i,0].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,0].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,0].set_aspect('equal')\n",
    "    ax[i,0].set_title(f\"Train {it}\")\n",
    "    a1 = ax[i,1].contourf(rr_train[it], zz_train[it], y_train[it], 20)\n",
    "    ax[i,1].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,1].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,1])\n",
    "    a2 = ax[i,2].contourf(rr_train[it], zz_train[it] ,-rhs_train[it], 20)\n",
    "    ax[i,2].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,2].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,2])\n",
    "    # evaluation\n",
    "    boxi = get_box_from_grid(rr_eval[ie], zz_eval[ie])\n",
    "    ax[i,3].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,3].plot(boxi[:,0], boxi[:,1])\n",
    "    ax[i,3].set_aspect('equal')\n",
    "    ax[i,3].set_title(f\"Eval {ie}\")\n",
    "    a1 = ax[i,4].contourf(rr_eval[ie], zz_eval[ie], y_eval[ie], 20)\n",
    "    ax[i,4].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,4].set_aspect('equal')\n",
    "    plt.colorbar(a1,ax=ax[i,4])\n",
    "    a2 = ax[i,5].contourf(rr_eval[ie], zz_eval[ie] ,-rhs_eval[ie], 20)\n",
    "    ax[i,5].plot(box0[:,0], box0[:,1])\n",
    "    ax[i,5].set_aspect('equal')\n",
    "    plt.colorbar(a2,ax=ax[i,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_ds_tf = tf.data.Dataset.from_tensor_slices((x_train, y_train, rhs_train[:,1:-1,1:-1], rr_train, zz_train, laplace_ker_t, df_dr_ker_t))\n",
    "eval_ds_tf = tf.data.Dataset.from_tensor_slices((x_eval, y_eval, rhs_eval[:,1:-1,1:-1], rr_eval, zz_eval, laplace_ker_e, df_dr_ker_e))\n",
    "# save the datasets\n",
    "tf.data.Dataset.save(train_ds_tf, join(DATA_DIR, 'train_ds_tf'))\n",
    "tf.data.Dataset.save(eval_ds_tf, join(DATA_DIR, 'eval_ds_tf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loading the datasets\n",
    "del train_ds_tf, eval_ds_tf\n",
    "train_ds_tf = tf.data.Dataset.load(join(DATA_DIR, 'train_ds_tf'))\n",
    "eval_ds_tf = tf.data.Dataset.load(join(DATA_DIR, 'eval_ds_tf'))\n",
    "#shuffle and batch the datasets\n",
    "train_ds_tf = train_ds_tf.shuffle(len(x_train)).batch(32)\n",
    "eval_ds_tf = eval_ds_tf.shuffle(len(x_eval)).batch(32)\n",
    "print(train_ds_tf.element_spec)\n",
    "print(eval_ds_tf.element_spec)\n",
    "print(f\"Training dataset size: {len(train_ds_tf)}\")\n",
    "print(f\"Eval dataset size: {len(eval_ds_tf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
