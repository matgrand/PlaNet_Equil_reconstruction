{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Prepapre dataset with the prepare_dataset notebook, before running this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")  # adds seaborn style to charts, eg. grid\n",
    "plt.style.use(\"dark_background\")  # inverts colors to dark theme\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "import os\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from utils import calc_gso_batch # gso/pinn calculation\n",
    "try: \n",
    "    JOBID = os.environ[\"SLURM_JOB_ID\"] # get job id from slurm, when training on cluster\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") # nvidia\n",
    "    HAS_SCREEN = False # for plotting or saving images\n",
    "except:\n",
    "    device = torch.device(\"mps\") # apple silicon\n",
    "    JOBID = \"local\"\n",
    "    HAS_SCREEN = True\n",
    "os.makedirs(f\"mg_data/{JOBID}\", exist_ok=True)\n",
    "print(f'device: {device}')\n",
    "\n",
    "def to_tensor(x, device=torch.device(\"cpu\")): return torch.tensor(x, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best MSE loss: 0.087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = f\"mg_data/{JOBID}\" \n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4*np.linspace(1, 1e-3, EPOCHS) \n",
    "GSO_LOSS_RATIO = np.linspace(0.5, 0.1, EPOCHS)\n",
    "# GSO_LOSS_RATIO = (0.5+0.5*np.sin(np.linspace(0, 25*np.pi, EPOCHS)))#*np.linspace(1, 0.1, EPOCHS) # crazy\n",
    "USE_CURRENTS = True\n",
    "USE_PROFILES = True\n",
    "USE_MAGNETIC = True\n",
    "INPUT_SIZE = int(USE_CURRENTS)*14 + int(USE_PROFILES)*202 + int(USE_MAGNETIC)*187\n",
    "TRAIN_DS_PATH = \"data/train_ds.mat\" # generated from prepapre_dataset\n",
    "EVAL_DS_PATH = \"data/eval_ds.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot schedulers: lr + gso loss ratio\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax[0].set_title(\"Learning Rate\")\n",
    "ax[0].plot(LEARNING_RATE)\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Learning Rate\")\n",
    "ax[1].set_title(\"GSO Loss Ratio\")\n",
    "ax[1].plot(GSO_LOSS_RATIO)\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"GSO Loss Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean current: -10183.76, std current: 34209.11\n",
    "- mean magnetic: -0.20, std magnetic: 0.58\n",
    "- mean f_profile: 33.13, std f_profile: 0.28\n",
    "- mean p_profile: 9654.42, std p_profile: 8788.29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_mat_path):\n",
    "        ds_mat = sio.loadmat(ds_mat_path)\n",
    "        # output: magnetic flux, transposed (matlab is column-major)\n",
    "        self.psi = to_tensor(ds_mat[\"psi\"]).view(-1, 1, 64, 64)\n",
    "        # inputs: radial and vertical position of pixels (for plotting only rn) + currents + measurements + profiles \n",
    "        self.rr = to_tensor(ds_mat[\"rr\"]).view(-1,1,64,64) # radial position of pixels (64, 64)\n",
    "        self.zz = to_tensor(ds_mat[\"zz\"]).view(-1,1,64,64) # vertical position of pixels (64, 64)\n",
    "        self.currs = ds_mat[\"currs\"] # input currents (n, 14)\n",
    "        self.magnetic = ds_mat[\"magnetic\"] # input magnetic measurements (n, 187)\n",
    "        self.f_profile = ds_mat[\"f_profiles\"] # input profiles (n, 101)\n",
    "        self.p_profile = ds_mat[\"p_profiles\"] # input profiles (n, 101)\n",
    "        inputs = [] # add the normalized inputs to the list\n",
    "        if USE_CURRENTS: inputs.append((to_tensor(self.currs)+10183)/34209) # (n, 14) # normalized\n",
    "        # if USE_CURRENTS: inputs.append((to_tensor(self.currs))) # (n, 14)\n",
    "        if USE_MAGNETIC: inputs.append((to_tensor(self.magnetic)+0.2)/0.58) # (n, 187) # normalized\n",
    "        # if USE_MAGNETIC: inputs.append((to_tensor(self.magnetic))) # (n, 187)\n",
    "        if USE_PROFILES: inputs.append(torch.cat(((to_tensor(self.f_profile)-33.13)/0.28, (to_tensor(self.p_profile)-9654)/8788), 1)) # (n, 202) # normalized\n",
    "        # if USE_PROFILES: inputs.append(torch.cat((to_tensor(self.f_profile), to_tensor(self.p_profile)), 1)) # (n, 202)\n",
    "        self.inputs = torch.cat(inputs, 1) # (n, 403)\n",
    "        #move to device (doable bc the dataset is fairly small, check memory usage)\n",
    "        self.psi, self.inputs, self.rr, self.zz = self.psi.to(device), self.inputs.to(device), self.rr.to(device), self.zz.to(device)\n",
    "        total_memory = sum([x.element_size()*x.nelement() for x in [self.psi, self.inputs, self.rr, self.zz]])\n",
    "        print(f\"Dataset: {len(self)}, memory: {total_memory/1024**2:.2f} MB\")\n",
    "    def __len__(self): return len(self.psi)\n",
    "    def __getitem__(self, idx): return self.inputs[idx], self.psi[idx], self.rr[idx], self.zz[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "print(f\"Dataset length: {len(ds)}\")\n",
    "print(f\"Input shape: {ds[0][0].shape}\")\n",
    "print(f\"Output shape: {ds[0][1].shape}\")\n",
    "n_plot = 10\n",
    "print(len(ds))\n",
    "fig, axs = plt.subplots(1, n_plot, figsize=(3*n_plot, 5))\n",
    "for i, j in enumerate(np.random.randint(0, len(ds), n_plot)):\n",
    "    psi, rr, zz = ds[j][1].cpu().numpy().squeeze(), ds[j][2].cpu().numpy().squeeze(), ds[j][3].cpu().numpy().squeeze()\n",
    "    axs[i].contourf(rr, zz, psi, 100, cmap=\"inferno\")\n",
    "    axs[i].contour(rr, zz, -psi, 20, colors=\"black\", linestyles=\"dotted\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_aspect(\"equal\")\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/dataset.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL: PlaNet: # Paper net: branch + trunk conenction and everything\n",
    "# from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, Upsample\n",
    "# class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "#     from torch.nn import Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU\n",
    "#     def __init__(self):\n",
    "#         super(PlaNet, self).__init__()\n",
    "#         af = ReLU() # activation function\n",
    "#         #branch\n",
    "#         self.branch = Sequential(\n",
    "#             Linear(INPUT_SIZE, 256), af,\n",
    "#             Linear(256, 128), af,\n",
    "#             Linear(128, 64), af\n",
    "#         )\n",
    "#         #trunk\n",
    "#         # def trunk_block(): # faster\n",
    "#         #     return  Sequential(\n",
    "#         #         Conv2d(1, 8, kernel_size=3, stride=2, padding=1), BatchNorm2d(8), af,\n",
    "#         #         Conv2d(8, 16, kernel_size=3, stride=2, padding=1), BatchNorm2d(16), af,\n",
    "#         #         Conv2d(16, 32, kernel_size=3, stride=2, padding=1), BatchNorm2d(32), af,\n",
    "#         #     )\n",
    "#         def trunk_block(): \n",
    "#             return  Sequential(\n",
    "#                 Conv2d(1, 8, kernel_size=3, stride=1, padding=1),  BatchNorm2d(8),  af, MaxPool2d(2),\n",
    "#                 Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16),  af, MaxPool2d(2),\n",
    "#                 Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), af, MaxPool2d(2),\n",
    "#             )\n",
    "#         self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "#         self.trunk_fc = Sequential(\n",
    "#             Linear(2*32*8*8, 128), af,\n",
    "#             Linear(128, 64), af,\n",
    "#             Linear(64, 64), af,\n",
    "#         )\n",
    "#         # head\n",
    "#         self.fc = Sequential(Linear(64, 2048), af)\n",
    "#         self.anti_conv = Sequential(\n",
    "#             Upsample(scale_factor=2, mode='bilinear'), Conv2d(32, 32, kernel_size=3, padding=1), BatchNorm2d(32), af, #NOTE: 32, not 64, paper inconsistency\n",
    "#             Upsample(scale_factor=2, mode='bilinear'), Conv2d(32, 16, kernel_size=3, padding=1), BatchNorm2d(16), af,\n",
    "#             Upsample(scale_factor=2, mode='bilinear'), Conv2d(16, 8, kernel_size=3, padding=1), BatchNorm2d(8), af,\n",
    "#             Conv2d(8, 1, kernel_size=3, padding=1)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         xb, r, z = x\n",
    "#         #branch net\n",
    "#         xb = self.branch(xb)\n",
    "#         #trunk net\n",
    "#         r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "#         r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "#         xt = torch.cat((r, z), 1) # concatenate\n",
    "#         xt = self.trunk_fc(xt) # fully connected\n",
    "#         # multiply trunk and branch\n",
    "#         x = xt * xb\n",
    "#         #head net\n",
    "#         x = self.fc(x)\n",
    "#         x = x.view(-1, 32, 8, 8)\n",
    "#         x = self.anti_conv(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: PlaNet: # Paper net: branch + trunk conenction and everything\n",
    "from torch.nn import Module, Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU, Sequential, Upsample, ConvTranspose2d\n",
    "class PlaNet(Module): # Paper net: branch + trunk conenction and everything\n",
    "    from torch.nn import Linear, Conv2d, MaxPool2d, BatchNorm2d, ReLU\n",
    "    def __init__(self):\n",
    "        super(PlaNet, self).__init__()\n",
    "        af = ReLU() # activation function\n",
    "        #branch\n",
    "        self.branch = Sequential(\n",
    "            Linear(INPUT_SIZE, 256), af,\n",
    "            Linear(256, 128), af,\n",
    "            Linear(128, 64), af\n",
    "        )\n",
    "        #trunk\n",
    "        # def trunk_block(): # faster\n",
    "        #     return  Sequential(\n",
    "        #         Conv2d(1, 8, kernel_size=3, stride=2, padding=1), BatchNorm2d(8), af,\n",
    "        #         Conv2d(8, 16, kernel_size=3, stride=2, padding=1), BatchNorm2d(16), af,\n",
    "        #         Conv2d(16, 32, kernel_size=3, stride=2, padding=1), BatchNorm2d(32), af,\n",
    "        #     )\n",
    "        def trunk_block(): \n",
    "            return  Sequential(\n",
    "                Conv2d(1, 8, kernel_size=3, stride=1, padding=1), BatchNorm2d(8), af, MaxPool2d(2),\n",
    "                Conv2d(8, 16, kernel_size=3, stride=1, padding=1), BatchNorm2d(16), af, MaxPool2d(2),\n",
    "                Conv2d(16, 32, kernel_size=3, stride=1, padding=1), BatchNorm2d(32), af, MaxPool2d(2),\n",
    "            )\n",
    "        self.trunk_r, self.trunk_z = trunk_block(), trunk_block()\n",
    "        self.trunk_fc = Sequential(\n",
    "            Linear(2*32*8*8, 128), af,\n",
    "            Linear(128, 64), af,\n",
    "            Linear(64, 64), af,\n",
    "        )\n",
    "        # head\n",
    "        self.fc = Sequential(Linear(64, 2048), af)\n",
    "        self.anti_conv = Sequential( # U-Net style\n",
    "            ConvTranspose2d(32, 32, kernel_size=2, stride=2), \n",
    "            Conv2d(32, 32, kernel_size=3, padding=0), af,\n",
    "            Conv2d(32, 32, kernel_size=3, padding=0), af,\n",
    "            ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            Conv2d(16, 16, kernel_size=3, padding=0), af,\n",
    "            Conv2d(16, 16, kernel_size=3, padding=0), af,\n",
    "            ConvTranspose2d(16, 8, kernel_size=2, stride=2),\n",
    "            Conv2d(8, 8, kernel_size=3, padding=0), af,\n",
    "            Conv2d(8, 8, kernel_size=3, padding=0), af,\n",
    "            ConvTranspose2d(8, 4, kernel_size=2, stride=2),\n",
    "            Conv2d(4, 2, kernel_size=3, padding=0), af,\n",
    "            Conv2d(2, 1, kernel_size=3, padding=0), af,\n",
    "            Conv2d(1, 1, kernel_size=5, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xb, r, z = x\n",
    "        #branch net\n",
    "        xb = self.branch(xb)\n",
    "        #trunk net\n",
    "        r, z = self.trunk_r(r), self.trunk_z(z) # convolutions\n",
    "        r, z = r.view(-1, 32*8*8), z.view(-1, 32*8*8) # flatten\n",
    "        xt = torch.cat((r, z), 1) # concatenate\n",
    "        xt = self.trunk_fc(xt) # fully connected\n",
    "        # multiply trunk and branch\n",
    "        x = xt * xb\n",
    "        #head net\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 32, 8, 8)\n",
    "        x = self.anti_conv(x)\n",
    "        return x\n",
    "    \n",
    "x = (torch.rand(1, INPUT_SIZE), torch.rand(1, 1, 64, 64), torch.rand(1, 1, 64, 64))\n",
    "net = PlaNet()\n",
    "y = net(x)\n",
    "print(f\"in: {[x.shape for x in x]}, out: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx = (torch.rand(n_sampl, INPUT_SIZE), torch.rand(n_sampl, 1, 64, 64), torch.rand(n_sampl, 1, 64, 64))\n",
    "ny = net(nx)\n",
    "print(f\"in: {[x.shape for x in nx]}, out: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, 64, 64), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        # self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        # self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        # self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        # self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        # self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        # self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        # self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        # self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        # self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (torch.rand(1, INPUT_SIZE), torch.rand(1, 1, 64, 64), torch.rand(1, 1, 64, 64))\n",
    "net = PlaNet()\n",
    "y = net(x)\n",
    "print(f\"in: {[x.shape for x in x]}, out: {y.shape}\")\n",
    "n_sampl = 7\n",
    "nx = (torch.rand(n_sampl, INPUT_SIZE), torch.rand(n_sampl, 1, 64, 64), torch.rand(n_sampl, 1, 64, 64))\n",
    "ny = net(nx)\n",
    "print(f\"in: {[x.shape for x in nx]}, out: {ny.shape}\")\n",
    "assert ny.shape == (n_sampl, 1, 64, 64), f\"Wrong output shape: {ny.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = PlaNetDataset(TRAIN_DS_PATH), PlaNetDataset(EVAL_DS_PATH) # initialize datasets\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "model = PlaNet()  # instantiate model\n",
    "model.to(device) # move model to device\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE[0])\n",
    "loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "tot_losses, mse_losses, gso_losses = [], [], [] # initialize losses\n",
    "start_time = time() # start time\n",
    "for ep in range(EPOCHS): \n",
    "    epoch_time = time()\n",
    "    for pg in optimizer.param_groups: pg['lr'] = LEARNING_RATE[ep] # update learning rate\n",
    "    model.train()\n",
    "    trainloss, evalloss = [], []\n",
    "    for input_currs, psi, rr, zz in train_dl:\n",
    "        # input_currs, psi, rr, zz = input_currs.to(device), psi.to(device), rr.to(device), zz.to(device) # move to device\n",
    "        optimizer.zero_grad() # zero gradients\n",
    "        psi_pred = model((input_currs, rr, zz)) # forward pass\n",
    "        gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device) # calculate grad shafranov\n",
    "        mse_loss = loss_fn(psi_pred, psi) # mean squared error loss on psi\n",
    "        gso_loss = loss_fn(gso_pred, gso) # PINN loss on grad shafranov\n",
    "        loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "        loss.backward() # backprop\n",
    "        optimizer.step() # update weights\n",
    "        trainloss.append((loss.item(), mse_loss.item(), gso_loss.item())) # save batch losses\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_currs, psi, rr, zz in val_dl:\n",
    "            # input_currs, psi, rr, zz = input_currs.to(device), psi.to(device), rr.to(device), zz.to(device) \n",
    "            psi_pred = model((input_currs, rr, zz))\n",
    "            gso, gso_pred = calc_gso_batch(psi, rr, zz, dev=device), calc_gso_batch(psi_pred, rr, zz, dev=device)\n",
    "            mse_loss = loss_fn(psi_pred, psi)\n",
    "            gso_loss = loss_fn(gso_pred, gso)\n",
    "            loss = (1-GSO_LOSS_RATIO[ep])*mse_loss + GSO_LOSS_RATIO[ep]*gso_loss # total loss\n",
    "            evalloss.append((loss.item(), mse_loss.item(), gso_loss.item()))\n",
    "    ttot_loss, tmse_loss, tgso_loss = map(lambda x: sum(x)/len(x), zip(*trainloss))\n",
    "    etot_loss, emse_loss, egso_loss = map(lambda x: sum(x)/len(x), zip(*evalloss))\n",
    "    # save model if improved        \n",
    "    endp = \"\\n\" \n",
    "    if etot_loss <= min(tot_losses, default=etot_loss): \n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_tot.pth\"); endp=\" *tot\\n\"\n",
    "    if emse_loss <= min(mse_losses, default=emse_loss):\n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_mse.pth\"); endp=\" *mse\\n\"\n",
    "    if egso_loss <= min(gso_losses, default=egso_loss):\n",
    "        torch.save(model.state_dict(), f\"{SAVE_DIR}/mg_planet_gso.pth\"); endp=\" *gso\\n\"\n",
    "    tot_losses.append(etot_loss); mse_losses.append(emse_loss); gso_losses.append(egso_loss) # save losses\n",
    "    print(f\"{ep+1}/{EPOCHS}: \"\n",
    "        #   f\"Train: {ttot_loss:.4f}, mse {tmse_loss:.4f}, gso {tgso_loss:.4f} | \" +\n",
    "          f\"Eval: tot {etot_loss:.4f}, mse {emse_loss:.4f}, gso {egso_loss:.4f} | \" + \n",
    "          f\"lr:{LEARNING_RATE[ep]:.1e}, r:{GSO_LOSS_RATIO[ep]:.2f} | {time()-epoch_time:.0f}s, eta:{(time()-start_time)*(EPOCHS-ep)/(ep+1)/60:.0f}m |\", end=endp,  flush=True)\n",
    "print(f\"Training time: {(time()-start_time)/60:.0f}mins\")\n",
    "print(f\"Best losses: tot {min(tot_losses):.3f}, mse {min(mse_losses):.3f}, gso {min(gso_losses):.3f}\")\n",
    "#save losses\n",
    "np.save(f\"{SAVE_DIR}/tot_losses.npy\", tot_losses)\n",
    "np.save(f\"{SAVE_DIR}/mse_losses.npy\", mse_losses)\n",
    "np.save(f\"{SAVE_DIR}/gso_losses.npy\", gso_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 6))\n",
    "ax[0,0].set_title(\"TOT Loss\")\n",
    "ax[0,0].plot(tot_losses)\n",
    "ax[0,0].set_xlabel(\"Epoch\")\n",
    "ax[0,0].set_ylabel(\"Loss\")\n",
    "ax[0,1].set_title(\"MSE Loss\")\n",
    "ax[0,1].plot(mse_losses)\n",
    "ax[0,1].set_xlabel(\"Epoch\")\n",
    "ax[0,1].set_ylabel(\"Loss\")\n",
    "ax[0,2].set_title(\"GSO Loss\")\n",
    "ax[0,2].plot(gso_losses)\n",
    "ax[0,2].set_xlabel(\"Epoch\")\n",
    "ax[0,2].set_ylabel(\"Loss\")\n",
    "#now the same but with log scale\n",
    "ax[1,0].set_title(\"TOT Loss (log)\")\n",
    "ax[1,0].plot(tot_losses)\n",
    "ax[1,0].set_xlabel(\"Epoch\")\n",
    "ax[1,0].set_ylabel(\"Loss\")\n",
    "ax[1,0].set_yscale(\"log\")\n",
    "ax[1,1].set_title(\"MSE Loss (log)\")\n",
    "ax[1,1].plot(mse_losses)\n",
    "ax[1,1].set_xlabel(\"Epoch\")\n",
    "ax[1,1].set_ylabel(\"Loss\")\n",
    "ax[1,1].set_yscale(\"log\")\n",
    "ax[1,2].set_title(\"GSO Loss (log)\")\n",
    "ax[1,2].plot(gso_losses)\n",
    "ax[1,2].set_xlabel(\"Epoch\")\n",
    "ax[1,2].set_ylabel(\"Loss\")\n",
    "ax[1,2].set_yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing network output\n",
    "for tit, best_model_path in zip([\"TOT\",\"MSE\", \"GSO\"], [\"mg_planet_tot.pth\", \"mg_planet_mse.pth\", \"mg_planet_gso.pth\"]):\n",
    "    model = PlaNet()\n",
    "    path = f\"{SAVE_DIR}/{best_model_path}\"\n",
    "    print(path)\n",
    "    model.load_state_dict(torch.load(f\"{SAVE_DIR}/{best_model_path}\"))\n",
    "    model.eval()\n",
    "    ds = PlaNetDataset(EVAL_DS_PATH)\n",
    "    # ds = PlaNetDataset(TRAIN_DS_PATH)\n",
    "    N_PLOTS = 1\n",
    "    for i in np.random.randint(0, len(ds), N_PLOTS):  \n",
    "        fig, axs = plt.subplots(2, 5, figsize=(20, 12))\n",
    "        input_currs, psi_ds, rr, zz = ds[i]\n",
    "        input_currs, psi_ds, rr, zz = input_currs.to('cpu'), psi_ds.to('cpu'), rr.to('cpu'), zz.to('cpu')\n",
    "        input_currs, psi_ds, rr, zz = input_currs.view(1,-1), psi_ds.view(1,1,64,64), rr.view(1,1,64,64), zz.view(1,1,64,64)\n",
    "        psi_pred = model((input_currs, rr, zz))\n",
    "        gso, gso_pred = calc_gso_batch(psi_ds, rr, zz), calc_gso_batch(psi_pred, rr, zz)\n",
    "        gso, gso_pred = gso.detach().numpy().reshape(64, 64), gso_pred.detach().numpy().reshape(64, 64)\n",
    "        gso_range = (gso.max(), gso.min())\n",
    "        gso_levels = np.linspace(gso_range[1], gso_range[0], 12)\n",
    "        gso_pred = np.clip(gso_pred, gso_range[1], gso_range[0]) # clip to gso range\n",
    "        \n",
    "        psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "        psi_ds = psi_ds.detach().numpy().reshape(64, 64)\n",
    "        rr, zz = rr.view(64, 64).detach().numpy(), zz.view(64, 64).detach().numpy()\n",
    "        ext = [ds.rr.min(), ds.rr.max(), ds.zz.min(), ds.zz.max()]\n",
    "        bmin, bmax = np.min([psi_ds, psi_pred]), np.max([psi_ds, psi_pred]) # min max psi\n",
    "        blevels = np.linspace(bmin, bmax, 13, endpoint=True)\n",
    "        ψ_mse = (psi_ds - psi_pred)**2\n",
    "        gso_mse = (gso - gso_pred)**2\n",
    "        mse_levels1 = np.linspace(0, 0.5, 13, endpoint=True)\n",
    "        mse_levels2 = np.linspace(0, 0.05, 13, endpoint=True)\n",
    "\n",
    "        im00 = axs[0,0].contourf(rr, zz, psi_ds, blevels, cmap=\"inferno\")\n",
    "        axs[0,0].set_title(\"Actual\")\n",
    "        axs[0,0].set_aspect('equal')\n",
    "        axs[0,0].set_ylabel(\"ψ\")\n",
    "        fig.colorbar(im00, ax=axs[0,0]) \n",
    "\n",
    "        im01 = axs[0,1].contourf(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,1].set_title(\"Predicted\")\n",
    "        axs[0,1].set_aspect('equal')\n",
    "        fig.colorbar(im01, ax=axs[0,1])\n",
    "\n",
    "        im02 = axs[0,2].contour(rr, zz, psi_ds, blevels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[0,2].contour(rr, zz, psi_pred, blevels, cmap=\"inferno\")\n",
    "        axs[0,2].set_title(\"Contours\")\n",
    "        axs[0,2].set_aspect('equal')\n",
    "        fig.colorbar(im02, ax=axs[0,2])\n",
    "\n",
    "        im03 = axs[0,3].contourf(rr, zz, np.clip(ψ_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[0,3].set_title(\"MSE 0.5\")\n",
    "        axs[0,3].set_aspect('equal')\n",
    "        fig.colorbar(im03, ax=axs[0,3])\n",
    "\n",
    "        im04 = axs[0,4].contourf(rr, zz, np.clip(ψ_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[0,4].set_title(\"MSE 0.05\")\n",
    "        axs[0,4].set_aspect('equal')\n",
    "        fig.colorbar(im04, ax=axs[0,4])\n",
    "\n",
    "        im10 = axs[1,0].contourf(rr, zz, gso, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,0].set_aspect('equal')\n",
    "        axs[1,0].set_ylabel(\"GSO\")\n",
    "        fig.colorbar(im10, ax=axs[1,0])\n",
    "\n",
    "        im6 = axs[1,1].contourf(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,1].set_aspect('equal')\n",
    "        fig.colorbar(im6, ax=axs[1,1])\n",
    "\n",
    "        im12 = axs[1,2].contour(rr, zz, gso, gso_levels, linestyles='dashed', cmap=\"inferno\")\n",
    "        axs[1,2].contour(rr, zz, gso_pred, gso_levels, cmap=\"inferno\")\n",
    "        axs[1,2].set_aspect('equal')\n",
    "        fig.colorbar(im12, ax=axs[1,2])\n",
    "\n",
    "        im13 = axs[1,3].contourf(rr, zz, np.clip(gso_mse, 0, 0.5), mse_levels1, cmap=\"inferno\")\n",
    "        axs[1,3].set_aspect('equal')\n",
    "        fig.colorbar(im13, ax=axs[1,3])\n",
    "\n",
    "        im14 = axs[1,4].contourf(rr, zz, np.clip(gso_mse, 0.00001, 0.04999), mse_levels2, cmap=\"inferno\")\n",
    "        axs[1,4].set_aspect('equal')\n",
    "        fig.colorbar(im14, ax=axs[1,4])\n",
    "\n",
    "        for ax in axs.flatten(): ax.grid(False), ax.set_xticks([]), ax.set_yticks([])\n",
    "\n",
    "        #suptitle\n",
    "        plt.suptitle(f\"PlaNet: {tit} {i}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show() if HAS_SCREEN else plt.savefig(f\"mg_data/{JOBID}/planet_{tit}_{i}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\", flush=True)\n",
    "sleep(30) # wait for files to update (for cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the log file to the folder\n",
    "os.system(f\"cp jobs/{JOBID}.txt mg_data/{JOBID}/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
