{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# device = torch.device(\"mps\") # for apple silicon, cpu is faster\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DS_PATH = \"mg_data/ITER_like_equilibrium_dataset_sample.mat\" # sample dataset\n",
    "FULL_DS_PATH = 'mg_data/ITER_like_equilibrium_dataset.mat' # full dataset\n",
    "MODEL_SAVE_PATH = \"mg_data/mg_planet.pth\"\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNetDataset(Dataset):\n",
    "    def __init__(self, ds_path):\n",
    "        ds = sio.loadmat(ds_path)\n",
    "        self.rr_pix = ds[\"RR_pixels\"] # radial position of pixels (64, 64)\n",
    "        self.zz_pix = ds[\"ZZ_pixels\"] # vertical position of pixels (64, 64)\n",
    "        self.input_currs = ds[\"DB_coils_curr_test_ConvNet\"] # input currents (n,14)\n",
    "        self.psi = ds[\"DB_psi_pixel_test_ConvNet\"] # magnetic flux (n, 64, 64)\n",
    "        # self.psi = self.psi.transpose(0, 2, 1).reshape(-1, 64*64) # transpose and flatten\n",
    "        self.psi = self.psi.transpose(0, 2, 1) # transpose (matlab is column major)\n",
    "        self.input_currs = torch.tensor(self.input_currs, dtype=torch.float32) # convert to tensor\n",
    "        self.psi = torch.tensor(self.psi, dtype=torch.float32) # convert to tensor\n",
    "    def __len__(self):\n",
    "        return len(self.psi)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_currs[idx], self.psi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PlaNet(torch.nn.Module): # simple fully connected neural network -> weak > loss:33\n",
    "#     def __init__(self):\n",
    "#         super(PlaNet, self).__init__()\n",
    "#         self.n = 8\n",
    "#         self.fc1 = torch.nn.Linear(14, self.n)\n",
    "#         self.fc2 = torch.nn.Linear(self.n, self.n)\n",
    "#         self.fc3 = torch.nn.Linear(self.n, 64*64)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         x = x.view(-1, 64, 64)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlaNet(torch.nn.Module): # transpose convolutional neural network -> stronger, fast, but artifacts > loss:0.49\n",
    "    def __init__(self):\n",
    "        super(PlaNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(14, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 64)\n",
    "        self.unconv1 = torch.nn.ConvTranspose2d(64, 32, kernel_size=3)\n",
    "        self.unconv2 = torch.nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2)\n",
    "        self.unconv3 = torch.nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2)\n",
    "        self.unconv4 = torch.nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2)\n",
    "        self.unconv5 = torch.nn.ConvTranspose2d(4, 1, kernel_size=4, stride=2)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = x.view(-1, 64, 1, 1)\n",
    "        x = torch.relu(self.unconv1(x))\n",
    "        x = torch.relu(self.unconv2(x))\n",
    "        x = torch.relu(self.unconv3(x))\n",
    "        x = torch.relu(self.unconv4(x))\n",
    "        x = self.unconv5(x)\n",
    "        x = x.view(-1, 64, 64)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PlaNet(torch.nn.Module): # upsample convolutional neural network -> slower -> but smoother\n",
    "#     def __init__(self):\n",
    "#         super(PlaNet, self).__init__()\n",
    "#         self.interp = 'bilinear' # 'nearest' or 'bilinear'\n",
    "#         self.fc1 = torch.nn.Linear(14, 32)\n",
    "#         self.fc2 = torch.nn.Linear(32, 64)\n",
    "#         self.unconv1 = torch.nn.Sequential(\n",
    "#             torch.nn.Upsample(scale_factor=4, mode=self.interp),\n",
    "#             torch.nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Upsample(scale_factor=2, mode=self.interp),\n",
    "#             torch.nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Upsample(scale_factor=2, mode=self.interp),\n",
    "#             torch.nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Upsample(scale_factor=2, mode=self.interp),\n",
    "#             torch.nn.Conv2d(8, 4, kernel_size=3, padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Upsample(scale_factor=2, mode=self.interp),\n",
    "#             torch.nn.Conv2d(4, 1, kernel_size=3, padding=1),\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = x.view(-1, 64, 1, 1)\n",
    "#         x = self.unconv1(x)\n",
    "#         x = x.view(-1, 64, 64)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PlaNet(torch.nn.Module): # mix of the previous 2, similar results, slow\n",
    "#     def __init__(self):\n",
    "#         super(PlaNet, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(14, 32)\n",
    "#         self.fc2 = torch.nn.Linear(32, 64)\n",
    "#         self.unconv1 = torch.nn.Sequential(\n",
    "#             torch.nn.ConvTranspose2d(64, 32, kernel_size=3),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ConvTranspose2d(4, 2, kernel_size=4, stride=2), # 4,2\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.ConvTranspose2d(2, 1, kernel_size=3, stride=2),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Conv2d(1, 1, kernel_size=3, stride=2)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = torch.relu(self.fc1(x))\n",
    "#         x = torch.relu(self.fc2(x))\n",
    "#         x = x.view(-1, 64, 1, 1)\n",
    "#         x = self.unconv1(x)\n",
    "#         x = x.view(-1, 64, 64)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = PlaNetDataset(FULL_DS_PATH), PlaNetDataset(SAMPLE_DS_PATH)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True) # initialize DataLoader\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)  \n",
    "model = PlaNet()  # instantiate model\n",
    "model.to(device) # move model to device\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.MSELoss() # Mean Squared Error Loss\n",
    "best_loss = float('inf') # initialize best loss\n",
    "for epoch in range(EPOCHS): \n",
    "    epoch_time = time()\n",
    "    model.train()\n",
    "    trainloss, evalloss = [], []\n",
    "    for input_currs, psi in tqdm(train_dl, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False):\n",
    "    # for input_currs, psi in train_dl:\n",
    "        input_currs, psi = input_currs.to(device), psi.to(device) # move to device\n",
    "        optimizer.zero_grad()\n",
    "        psi_pred = model(input_currs)\n",
    "        loss = loss_fn(psi_pred, psi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainloss.append(loss.item())\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input_currs, psi in val_dl:\n",
    "            input_currs, psi = input_currs.to(device), psi.to(device) # move to device\n",
    "            psi_pred = model(input_currs)\n",
    "            loss = loss_fn(psi_pred, psi)\n",
    "            evalloss.append(loss.item())\n",
    "    print(f\"Ep {epoch+1}: Train Loss: {sum(trainloss)/len(trainloss):.4f}, Eval Loss: {sum(evalloss)/len(evalloss):.4f}, Time: {time()-epoch_time:.2f}s,\", end=\" \")\n",
    "    if sum(evalloss)/len(evalloss) < best_loss:\n",
    "        best_loss = sum(evalloss)/len(evalloss)\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlaNet()\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.eval()\n",
    "ds = PlaNetDataset(SAMPLE_DS_PATH)\n",
    "for i in np.random.randint(0, len(ds), 10):  \n",
    "    input_currs, psi = ds[i]\n",
    "    psi_pred = model(input_currs.unsqueeze(0))\n",
    "    psi_pred = psi_pred.detach().numpy().reshape(64, 64)\n",
    "    psi = psi.detach().numpy().reshape(64, 64)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    ext = [ds.rr_pix.min(), ds.rr_pix.max(), ds.zz_pix.min(), ds.zz_pix.max()]\n",
    "    rr, zz = ds.rr_pix, ds.zz_pix  # radial and vertical positions of pixels\n",
    "    bmin, bmax = np.min([psi, psi_pred]), np.max([psi, psi_pred])\n",
    "    err = np.abs(psi - psi_pred)*100/bmax\n",
    "\n",
    "    im0 = axs[0].imshow(psi, extent=ext, vmin=bmin, vmax=bmax)\n",
    "    axs[0].set_title(\"Actual\")\n",
    "    axs[0].set_aspect('equal')\n",
    "    fig.colorbar(im0, ax=axs[0]) \n",
    "\n",
    "    im1 = axs[1].imshow(psi_pred, extent=ext, vmin=bmin, vmax=bmax)\n",
    "    axs[1].set_title(\"Predicted\")\n",
    "    axs[1].set_aspect('equal')\n",
    "    fig.colorbar(im1, ax=axs[1])\n",
    "\n",
    "    im2 = axs[2].imshow(err, extent=ext)\n",
    "    axs[2].set_title(\"Error %\")\n",
    "    axs[2].set_aspect('equal')\n",
    "    fig.colorbar(im2, ax=axs[2])\n",
    "\n",
    "    c0 = axs[3].contour(rr, zz, psi, levels=15, cmap='viridis', linestyles='dashed')\n",
    "    c1 = axs[3].contour(rr, zz, psi_pred, levels=10, cmap='viridis')\n",
    "    axs[3].set_title(\"Contours\")\n",
    "    axs[3].set_aspect('equal')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
