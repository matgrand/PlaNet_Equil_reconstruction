{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4-QHFCjANoh",
        "outputId": "b138cf2a-1e71-461d-b8a3-b6b432d1f72b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../') # add parent directory to path\n",
        "from src.train.utils_train import calc_laplace_df_dr_ker\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import io\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from matplotlib import cm\n",
        "import h5py\n",
        "DTYPE = 'float32'\n",
        "filename = '../data/ITER_like_equilibrium_dataset.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # download datasets from gdrive, # uncomment if you want to download the dataset\n",
        "# import gdown\n",
        "# gdown.download(id=\"1-5KP7_OYIvDD_QXvIr5sDihVxZx1qJCN\", output='data/ITER_like_equilibrium_dataset_sample.mat', quiet=False)\n",
        "# gdown.download(id=\"1Gn_OrMzxPRkTk-i77--HiWmWZyd8i8ue\", output='data/ITER_like_equilibrium_dataset.mat', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # generate h5 file from downloaded mat file # uncomment if needed\n",
        "# data = io.loadmat('../data/ITER_like_equilibrium_dataset.mat')\n",
        "# with h5py.File(filename,'w') as handle:\n",
        "#     for k,v in data.items():\n",
        "#         if '__' not in k:\n",
        "#             print(k)\n",
        "#             chunks = [1]\n",
        "#             chunks.extend(list(v.shape[1:]))\n",
        "#             handle.create_dataset(k, data=v, shape=v.shape, chunks=tuple(chunks), dtype='float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_sample_train = 35000\n",
        "\n",
        "with h5py.File(filename, 'r') as handle:\n",
        "    DB_psi_pixel_test_ConvNet = handle['DB_psi_pixel_test_ConvNet'][()].astype(DTYPE)\n",
        "    DB_meas_Bpickup_test_ConvNet = handle['DB_meas_Bpickup_test_ConvNet'][()].astype(DTYPE)\n",
        "    DB_coils_curr_test_ConvNet = handle['DB_coils_curr_test_ConvNet'][()].astype(DTYPE)\n",
        "    DB_p_test_ConvNet = handle['DB_p_test_ConvNet'][()].astype(DTYPE)\n",
        "    RR_pixels = handle['RR_pixels'][()].astype(DTYPE)\n",
        "    ZZ_pixels = handle['ZZ_pixels'][()].astype(DTYPE)\n",
        "    DB_res_RHS_pixel_test_ConvNet = handle['DB_res_RHS_pixel_test_ConvNet'][()].astype(DTYPE)\n",
        "    DB_separatrix_200_test_ConvNet = handle['DB_separatrix_200_test_ConvNet'][()].astype(DTYPE)\n",
        "    DB_Jpla_pixel_test_ConvNet = handle['DB_Jpla_pixel_test_ConvNet'][()].astype(DTYPE)\n",
        "y_data = DB_psi_pixel_test_ConvNet\n",
        "X_data = np.column_stack((DB_meas_Bpickup_test_ConvNet, DB_coils_curr_test_ConvNet)) #, DB_f_test_ConvNet, DB_p_test_ConvNet ))\n",
        "# Save RR_pixels, ZZ_pixels\n",
        "outdir = '../data/'\n",
        "dict_save = {'RR_pixels':RR_pixels,'ZZ_pixels':ZZ_pixels}\n",
        "nr,nz = RR_pixels.shape\n",
        "io.savemat('{}data_geo_Dataset_NeuralOpt_super_res_{}x{}.mat'.format(outdir,nr,nz),dict_save)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "skegHPzf_uoi",
        "outputId": "5eed5ee8-9b06-4ca2-9686-3fad8f9f3629"
      },
      "outputs": [],
      "source": [
        "# plot dataset example\n",
        "for i in range(0,1):\n",
        "    ind_plot = np.random.randint(0,DB_Jpla_pixel_test_ConvNet.shape[0],1)[0]\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n",
        "    img = axs[0].contour(RR_pixels,ZZ_pixels,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15)\n",
        "    fig.colorbar(img)\n",
        "    axs[0].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')\n",
        "    axs[0].axis('equal')\n",
        "    axs[0].set_xlabel('r [m]')\n",
        "    axs[0].set_ylabel('z [m]')\n",
        "    axs[0].set_title('Ψ [Wb] - equil. #{}'.format(ind_plot))\n",
        "    img = axs[1].contourf(RR_pixels,ZZ_pixels,DB_Jpla_pixel_test_ConvNet[ind_plot,:,:],15)\n",
        "    fig.colorbar(img)\n",
        "    axs[1].axis('equal')\n",
        "    axs[1].set_title('$J_Ψ$ [A/m2] - equil. #{}'.format(ind_plot))\n",
        "    axs[1].set_xlabel('r [m]')\n",
        "    axs[1].set_ylabel('z [m]')\n",
        "    axs[1].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0],DB_separatrix_200_test_ConvNet[ind_plot,:,1],c='g')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot Ψ and GS operator\n",
        "cmap = cm.inferno\n",
        "for i in range(0,1):\n",
        "    ind_plot = np.random.randint(0,DB_Jpla_pixel_test_ConvNet.shape[0],1)[0]\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=True)\n",
        "    img = axs[0].contourf(RR_pixels,ZZ_pixels,DB_psi_pixel_test_ConvNet[ind_plot,:,:],15,cmap=cmap)\n",
        "    axs[0].plot( DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
        "    axs[0].set_xlim([RR_pixels.min(),RR_pixels.max()])\n",
        "    axs[0].set_ylim([ZZ_pixels.min(),ZZ_pixels.max()])\n",
        "    axs[0].axis('equal')\n",
        "    axs[0].set_axis_off()\n",
        "    axs[0].set_title('Ψ')\n",
        "    qq = DB_res_RHS_pixel_test_ConvNet[ind_plot,:,:]\n",
        "    img = axs[1].contourf(RR_pixels,ZZ_pixels,qq,15,cmap=cmap)\n",
        "    axs[1].set_title('GS operator')\n",
        "    axs[1].axis('equal')\n",
        "    axs[1].set_xlim([RR_pixels.min(),RR_pixels.max()])\n",
        "    axs[1].set_ylim([ZZ_pixels.min(),ZZ_pixels.max()])\n",
        "    axs[1].axis('equal')\n",
        "    axs[1].plot(DB_separatrix_200_test_ConvNet[ind_plot,:,0], DB_separatrix_200_test_ConvNet[ind_plot,:,1], c='g')\n",
        "    axs[1].set_axis_off()\n",
        "    # plt.savefig('./figures/equil_and_GSope', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "### train-test split\n",
        "id_train = np.load('{}/id_train.npy'.format(outdir))\n",
        "id_test = np.load('{}/id_test.npy'.format(outdir))\n",
        "### Standardize input data\n",
        "scaler = StandardScaler()\n",
        "X_data = scaler.fit_transform(X_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils functions\n",
        "import numpy as np\n",
        "from scipy.interpolate import RegularGridInterpolator\n",
        "\n",
        "def sample_random_subgrids(RR_pixels,ZZ_pixels,nr=64,nz=64):\n",
        "  delta_r_min = .33*(RR_pixels.max()-RR_pixels.min())\n",
        "  delta_r_max = .75*(RR_pixels.max()-RR_pixels.min())\n",
        "  delta_z_min = .2*(ZZ_pixels.max()-ZZ_pixels.min())\n",
        "  delta_z_max = .75*(ZZ_pixels.max()-ZZ_pixels.min())\n",
        "  delta_r = np.random.uniform(delta_r_min,delta_r_max,1)\n",
        "  r0 = np.random.uniform(RR_pixels.min(),RR_pixels.min()+delta_r_max-delta_r,1)\n",
        "  delta_z = np.random.uniform(delta_z_min,delta_z_max,1)\n",
        "  z0 = np.random.uniform(ZZ_pixels.min(),ZZ_pixels.min()+delta_z_max-delta_z,1)\n",
        "  rr = np.linspace(r0,r0+delta_r,nr)\n",
        "  zz = np.linspace(z0,z0+delta_z,nz)\n",
        "  rr_grid, zz_grid = np.meshgrid(rr,zz,indexing='xy')\n",
        "  return rr_grid, zz_grid\n",
        "\n",
        "def get_box_from_grid(rr_grid, zz_grid):\n",
        "  rm, rM, zm, zM = rr_grid.min(), rr_grid.max(), zz_grid.min(), zz_grid.max()\n",
        "  return np.array([[rm,zm],[rM,zm],[rM,zM],[rm,zM],[rm,zm]])\n",
        "\n",
        "def interp_fun(f, RR_pixels, ZZ_pixels, rr_grid, zz_grid, kind='quintic'):\n",
        "  interp_func = RegularGridInterpolator((RR_pixels[0,:], ZZ_pixels[:,0]), f.T, method=kind)\n",
        "  pts = np.column_stack((rr_grid.flatten(), zz_grid.flatten()))\n",
        "  f_int = interp_func(pts).reshape(rr_grid.shape)\n",
        "  return f_int\n",
        "\n",
        "def def_grids_and_interp(f, rhs, RR_pixels, ZZ_pixels, nr=64, nz=64, kind='quintic'):\n",
        "  rr_grid, zz_grid = sample_random_subgrids(RR_pixels,ZZ_pixels,nr,nz)\n",
        "  f_grid = interp_fun(f,RR_pixels,ZZ_pixels,rr_grid, zz_grid,kind=kind)\n",
        "  rhs_grid = interp_fun(rhs,RR_pixels,ZZ_pixels,rr_grid, zz_grid,kind=kind)\n",
        "  return rr_grid, zz_grid, f_grid, rhs_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "YtetovJywsby",
        "outputId": "fcccf8de-5995-4623-83ee-fb666f7f9b3d"
      },
      "outputs": [],
      "source": [
        "f = y_data[0,:,:]\n",
        "rr_grid, zz_grid = sample_random_subgrids(RR_pixels,ZZ_pixels)\n",
        "box = get_box_from_grid(rr_grid, zz_grid)\n",
        "f_grid = interp_fun(f, RR_pixels, ZZ_pixels, rr_grid, zz_grid, kind='quintic')\n",
        "\n",
        "fig,ax = plt.subplots(1,3)\n",
        "ax[0].scatter(RR_pixels.ravel(), ZZ_pixels.ravel(), marker='.')\n",
        "ax[0].scatter(rr_grid.ravel(), zz_grid.ravel(), marker='.')\n",
        "ax[0].set_aspect('equal', 'box')\n",
        "\n",
        "im = ax[1].contourf(RR_pixels, ZZ_pixels, f, 20)\n",
        "ax[1].set_aspect('equal', 'box')\n",
        "ax[1].plot(box[:,0],box[:,1])\n",
        "plt.colorbar(im,ax=ax[1])\n",
        "\n",
        "im = ax[2].contourf(rr_grid, zz_grid, f_grid, 20)\n",
        "ax[2].set_aspect('equal', 'box')\n",
        "ax[2].plot(box[:,0],box[:,1])\n",
        "plt.colorbar(im,ax=ax[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugc2tGbQp6mD",
        "outputId": "6f161b34-9945-49c4-df59-071b20864ed3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The full training dataset is ~60k equilibria. We select 35k equilibria on the full grid\n",
        "and 35k equilibria to be resampled on randomly generated sub-grids. These two datasets \n",
        "are then put together ans shuffled.\n",
        "\n",
        "_all_domain -> quantities related to the original grid\n",
        "_super_res -> quantities related to random sub-grids\n",
        "\n",
        "It woul be nice to explore how the performance changes using data different from the \n",
        "aforementioned 35k/35k.\n",
        "'''\n",
        "np.random.seed(42)\n",
        "ind_all_domain = np.random.choice(id_train,n_sample_train)\n",
        "ind_super_res = np.random.choice(id_train,n_sample_train)\n",
        "\n",
        "n_all_domain = ind_all_domain.shape[0]\n",
        "n_super_res = ind_super_res.shape[0]\n",
        "\n",
        "print(f'{n_all_domain} equil on the original grid')\n",
        "print(f'{n_super_res} equil on random sub-grids')\n",
        "\n",
        "y_train_all_domain = y_data[ind_all_domain,:,:]\n",
        "X_train_all_domain = X_data[ind_all_domain,:]\n",
        "res_RHS_pixel_train_all_domain = DB_res_RHS_pixel_test_ConvNet[ind_all_domain,:,:]\n",
        "\n",
        "y_test_all_domain = y_data[id_test,:,:]\n",
        "X_test_all_domain = X_data[id_test,:]\n",
        "res_RHS_pixel_test_all_domain = DB_res_RHS_pixel_test_ConvNet[id_test,:,:]\n",
        "\n",
        "f_for_super_res = y_data[ind_super_res,:,:]\n",
        "rhs_for_super_res = DB_res_RHS_pixel_test_ConvNet[ind_super_res,:,:]\n",
        "\n",
        "X_train_super_res = X_data[ind_super_res,:]\n",
        "y_train_super_res = np.zeros((n_super_res,64,64), dtype=DTYPE)\n",
        "rhs_train_super_res = np.zeros_like(y_train_super_res, dtype=DTYPE)\n",
        "RR_grid_super_res = np.zeros_like(y_train_super_res, dtype=DTYPE)\n",
        "ZZ_grid_super_res = np.zeros_like(y_train_super_res, dtype=DTYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# interpolate f and rhs on random sub-grids, NOTE: very slow if using 'quintic' method\n",
        "t_start = time()\n",
        "for i in tqdm(range(f_for_super_res.shape[0]), miniters = 0):\n",
        "    rr_grid, zz_grid, f_grid, rhs_grid = def_grids_and_interp(f_for_super_res[i,...],rhs_for_super_res[i,...],RR_pixels,ZZ_pixels, kind='quintic')\n",
        "    RR_grid_super_res[i,:,:] = rr_grid\n",
        "    ZZ_grid_super_res[i,:,:] = zz_grid\n",
        "    y_train_super_res[i,:,:] = f_grid\n",
        "    rhs_train_super_res[i,:,:] = rhs_grid\n",
        "t_elapsed = time() - t_start\n",
        "print(f't_elapsed = {t_elapsed}, time_per_step = {t_elapsed/n_super_res}')\n",
        "RR_grid_all_domain = np.tile(RR_pixels,(n_all_domain,1,1))\n",
        "ZZ_grid_all_domain = np.tile(ZZ_pixels,(n_all_domain,1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Check re-sampled equils\n",
        "import random\n",
        "i = random.randint(0,n_super_res)\n",
        "\n",
        "f = f_for_super_res[i,:,:]\n",
        "rhs = rhs_for_super_res[i,:,:]\n",
        "rr_grid = RR_grid_super_res[i,:,:]\n",
        "rr_grid = RR_grid_super_res[i,:,:]\n",
        "rr_grid = RR_grid_super_res[i,:,:]\n",
        "rr_grid = RR_grid_super_res[i,:,:]\n",
        "box = get_box_from_grid(rr_grid, zz_grid)\n",
        "\n",
        "fig,ax = plt.subplots(1,3)\n",
        "ax[0].scatter(RR_pixels.ravel(), ZZ_pixels.ravel(), marker='.')\n",
        "# ax[0].scatter(rr_grid.ravel(), zz_grid.ravel(), marker='.')\n",
        "ax[0].plot(box[:,0],box[:,1],c='r')\n",
        "ax[0].set_aspect('equal', 'box')\n",
        "im = ax[1].contourf(RR_pixels, ZZ_pixels, f, 20)\n",
        "ax[1].set_aspect('equal', 'box')\n",
        "ax[1].plot(box[:,0],box[:,1],c='r')\n",
        "plt.colorbar(im,ax=ax[1])\n",
        "im = ax[2].contourf(rr_grid, zz_grid, f_grid, 20)\n",
        "ax[2].set_aspect('equal', 'box')\n",
        "ax[2].plot(box[:,0],box[:,1])\n",
        "plt.colorbar(im,ax=ax[2])\n",
        "plt.show()\n",
        "\n",
        "fig,ax = plt.subplots(1,3)\n",
        "ax[0].scatter(RR_pixels.ravel(), ZZ_pixels.ravel(), marker='.')\n",
        "# ax[0].scatter(rr_grid.ravel(), zz_grid.ravel(), marker='.')\n",
        "ax[0].plot(box[:,0],box[:,1],c='r')\n",
        "ax[0].set_aspect('equal', 'box')\n",
        "im = ax[1].contourf(RR_pixels, ZZ_pixels, rhs, 20)\n",
        "ax[1].set_aspect('equal', 'box')\n",
        "ax[1].plot(box[:,0],box[:,1],c='r')\n",
        "plt.colorbar(im,ax=ax[1])\n",
        "im = ax[2].contourf(rr_grid, zz_grid, rhs_grid, 20)\n",
        "ax[2].set_aspect('equal', 'box')\n",
        "ax[2].plot(box[:,0],box[:,1])\n",
        "plt.colorbar(im,ax=ax[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute kernels for the GS equation train\n",
        "from src.train.utils_train import calc_laplace_df_dr_ker\n",
        "''' Compute kernels for GS Equation for the qeuils on the sub-grids (dr,dz is grid-dependent), \n",
        "otherwise we would have to compute them on the fly during each training step '''\n",
        "y_train = np.row_stack([y_train_all_domain,y_train_super_res])\n",
        "X_train = np.row_stack([X_train_all_domain,X_train_super_res])\n",
        "res_RHS_pixel_train = np.row_stack([res_RHS_pixel_train_all_domain,rhs_train_super_res])\n",
        "RR_pixel_train = np.row_stack([RR_grid_all_domain,RR_grid_super_res])\n",
        "ZZ_pixel_train = np.row_stack([ZZ_grid_all_domain,ZZ_grid_super_res])\n",
        "print(X_train.shape, y_train.shape, res_RHS_pixel_train[:,1:-1,1:-1].shape, RR_pixel_train.shape, ZZ_pixel_train.shape)\n",
        "Laplace_kernel = np.zeros((y_train.shape[0],3,3),dtype=DTYPE)\n",
        "Df_dr_kernel = np.zeros((y_train.shape[0],3,3),dtype=DTYPE)\n",
        "hrs, hzs = RR_pixel_train[:,1,2] - RR_pixel_train[:,1,1], ZZ_pixel_train[:,2,1] - ZZ_pixel_train[:,1,1]\n",
        "for i, (hr, hz) in enumerate(tqdm(zip(hrs, hzs), total=len(hrs))):\n",
        "    Laplace_kernel[i,:,:], Df_dr_kernel[i,:,:] = calc_laplace_df_dr_ker(hr, hz)\n",
        "### Dataset with equilibria on the entire domain\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train, res_RHS_pixel_train[:,1:-1,1:-1], RR_pixel_train, ZZ_pixel_train, Laplace_kernel, Df_dr_kernel)).shuffle(42)#.batch(batch_size=1024)\n",
        "tf.data.Dataset.save(train_ds, '{}tf_Dataset_train_NeuralOpt_{}x{}_{}_samples.data'.format(outdir,nr,nz, X_train.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute kernels for the GS equation test\n",
        "y_test = y_test_all_domain\n",
        "X_test = X_test_all_domain\n",
        "res_RHS_pixel_test = res_RHS_pixel_test_all_domain\n",
        "RR_pixel_test = np.tile(RR_pixels,(X_test.shape[0],1,1))\n",
        "ZZ_pixel_test = np.tile(ZZ_pixels,(X_test.shape[0],1,1))\n",
        "Laplace_kernel = np.zeros((y_test.shape[0],3,3),dtype=DTYPE)\n",
        "Df_dr_kernel = np.zeros((y_test.shape[0],3,3),dtype=DTYPE)\n",
        "hrs, hzs = RR_pixel_test[:,1,2] - RR_pixel_test[:,1,1], ZZ_pixel_test[:,2,1] - ZZ_pixel_test[:,1,1]\n",
        "for i, (hr, hz) in enumerate(tqdm(zip(hrs, hzs), total=len(hrs))):\n",
        "    Laplace_kernel[i,:,:], Df_dr_kernel[i,:,:] = calc_laplace_df_dr_ker(hr, hz)\n",
        "# Dataset with equilibria on the entire domain\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(( X_test, y_test, res_RHS_pixel_test[:,1:-1,1:-1], RR_pixel_test, ZZ_pixel_test, Laplace_kernel, Df_dr_kernel)).shuffle(42)\n",
        "tf.data.Dataset.save(train_ds, '{}tf_Dataset_test_NeuralOpt_{}x{}_{}_samples.data'.format(outdir,nr,nz, y_test.shape[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
